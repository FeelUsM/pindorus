{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Зачем всё это?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Есть машинный перевод и есть автоматизированный перевод. \n",
    "\n",
    "Машинный перевод дает 100% автоматизации и не позволяет вмешиваться в процесс перевода (только в результат). Конечно у многих машинных переводчиков есть функция \"предложить перевод\", но я не понимаю, как она работает, и работает ли хоть как-то вообще. Однажды я всё-таки заметил, как это работает: в яндекс-переводчике предложил перевод слова (уже не помню какого), и через сутки до него дошло, как надо переводить это слово. Через сутки, Карл!\n",
    "\n",
    "Автоматизированный перевод дает где-то 15-20% автоматизации. Помимо словарей, глоссариев и прочей справочной информации, самая продвинутая (известная мне) технология - это память переводов, когда человек вручную переводит предложения, а система запоминает эти переводы, и если встречается предложение, которое было переведено раньше (или _похожее_ на него), его перевод подставляется автоматически. Но какова вероятность встретить в тексте 2 одинаковых предложения, если в них больше трёх слов?\n",
    "\n",
    "Целью данного переводчика является автоматизация 90%. Не 100 и не 20. А также мгновенное вступление изменений в силу. Ну и возможность залезть в код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Основной принцип"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "p_паттерн парсит текст (str, pos), \n",
    "    вызывая другие паттерны, возвращающие древовидные структуры\n",
    "    эти древовидные структуры передает одному из правил\n",
    "        (остальные вызовы правил закомментированы \n",
    "            - в рамках одного текста каждый паттерн имеет ровно 1 смысл)\n",
    "    и возвращает (pos, результат этого правила), помещенный в массив\n",
    "    \n",
    "    /*пока на ошибках парсинга концентрировать не будем*/\n",
    "    если ничего не удалось распарсить, ???возвращаемый массив будет пустым???\n",
    "    если удалось распарсить несколько вариантов - в массиве будет насколько вариантов\n",
    "        сначала парсятся все исключения\n",
    "        потом парсятся все обычные варианты (если нет исключений)\n",
    "```\n",
    "\n",
    "```\n",
    "r_правило получает список древовидных структур\n",
    "    обрабатывает их по определенному правилу\n",
    "    возвращает древовидную структуру\n",
    "```\n",
    "\n",
    "```\n",
    "древовидная структура - map, со следующими ключами\n",
    "    type: 'noun'/'adj'/'verb'/'num'/... - определяем через isinstance()\n",
    "    постоянные параметры (для сущ.: род, число)\n",
    "    переменные параметры (для сущ.: падеж)\n",
    "    talk: массив древовидных структур\n",
    "        или пар (тип, слово), где тип - main/punct/other\n",
    "```\n",
    "\n",
    "```\n",
    "str(древовидная структура)\n",
    "    превращает древовидную структуру в строку\n",
    "```\n",
    "\n",
    "```\n",
    "для каждого типа элемента древовидной структуры будет map\n",
    "    где каждой строке (для сущ. - слово в и.п.) будет соответствовать\n",
    "        структура данных, которая вместе в переменными параметрами передается в \n",
    "        sh_функция для этого типа элемента древовидной структуры\n",
    "            которая будет возвращать слово в соотв. форме (для сущ. в соотв. падеже)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что с этим делать дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "В дальнейшем предполагается, что паттерны и правила будут писать пользователи\n",
    "грамматика не преобразовывается в LL(1) или какой-то другой промежуточный формат,\n",
    "а парсится как есть, с возвратами (LL(*)), поэтому результаты нетерминалов кэшируются.\n",
    "Также стоит защита от зацикливания.\n",
    "Никто не обещал, что грамматика будет однозначной, \n",
    "поэтому каждый нетерминал возвращает массив результатов.\n",
    "Но в конечном итоге таких результатов должно быть немного.\n",
    "Ситуация, когда получаются одинаковые результаты явлется нежелательной.\n",
    "\n",
    "В дальнейшем предполагается, что будет центральная грамматика, \n",
    "а у ее правил пользователи будут создавать исключения и расширения.\n",
    "Фишка в том, что приредактировании грамматики этим способом \n",
    "    поведение грамматики для уже имеющихся тестов/текстов не изменится.\n",
    "Возможно периодически для оптимизации грамматики будет требоваться полная ее переработка,\n",
    "но чисто математическая и довольно вычислительно-сложная задача.\n",
    "Впрочем и без оптимизации производительность ухудшается не сильно.\n",
    "\n",
    "Паттерн A является исключением паттерна B, если \n",
    "всё что может разобрать паттерн A может разобрать паттерн B,\n",
    "т.е. A задает подъязык языка B, \n",
    "т.е. A является частным случаем B (но связано с другим правилом).\n",
    "Сначала парсится B, и если это оказалось удачным, парсится A.\n",
    "Если А распарсилось неудачно, то результатом станосится результат B,\n",
    "а если удачно - то результат B отбрасывается и результатом станосится результат A.\n",
    "\n",
    "Паттерн C является расширением паттерна B, если\n",
    "...\n",
    "\n",
    "В дальнейшем предполагается возможность каждый паттерн связывать с \n",
    "набором правил, а точнее с одним правилом из заданного набора.\n",
    "А также возможность эти наборы пополнять.\n",
    "Одним из правил перевода исключения будет вариант, когда\n",
    "результат исключения отбрасывается а результатом становится \n",
    "результат правила регулярного паттерна.\n",
    "Это дает возможность не сломать уже имеющийся перевод из-за добавления исключений к грамматике.\n",
    "\n",
    "Вопрос дефолтного связывания паттернов с правилами допускает множество решений\n",
    "и остается открытым.\n",
    "В любом случае пользователь сможет создавать исключения паттернов и дополнительные правила,\n",
    "тем самым пополняя базу данных переводчика,\n",
    "а также менять связи паттернов с правилами для своего текста, сохранять эти связи,\n",
    "и применять к другим текстам.\n",
    "\n",
    "паттерн - набор альтернатив\n",
    "альтернатива - последовательность, с которой связан набор правил\n",
    "набор правил - набор правил + номер дефолтного правила\n",
    "\t\tили просто правило\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Организация кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* parse_system.py - низкоуровневые классы, токенезация и базовые функции парсинга\n",
    "* classes.py - классы частей речи\n",
    "* ru_dictionary.py - функции отображения, русские слова и функции их добавления\n",
    "* en_dictionary.py - словарь\n",
    "\n",
    "\n",
    "* en2ru.ipynb .py - описание всего, грамматика, маленькие тесты, todo\n",
    "* tests.ipynb - тесты уже имеющихся переводов\n",
    "* utils.ipynb - прочее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:49.911798Z",
     "start_time": "2019-04-16T20:05:49.895805Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''англо-русский переводчик, основанный на правилах, с простым добавлением паттернов и правил\n",
    "\n",
    "en2ru(s) -> str - переводит строку, возвращает строку\n",
    "d_en2ru(s) -> str - дополнительно печатает отладочный вывод\n",
    "pr_l_repr(s) - печатает строку в тройных кавычках\n",
    "decline(s,pads=['ip','rp','dp','vp','tp','pp']) - возвращает список склонений переведенной фразы\n",
    "parse_pat(patt,s) -> Struct - парсит строку паттерном, возвращает нестрингифицированный объект\n",
    "d_parse_pat(patt,s) -> Struct - дополнительно печатает отладочный вывод\n",
    "scheme(s) - печатает схему разбора строки\n",
    "\n",
    "паттерны:\n",
    "p_text\n",
    "p_sentence\n",
    "p_phrase\n",
    "p_verb\n",
    "...\n",
    "p_noun\n",
    "...\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.155807Z",
     "start_time": "2019-04-16T20:05:49.927805Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import parse_system\n",
    "from parse_system import S, SAttrs, ParseInfo, tokenizer, tokenize,\\\n",
    "                        ch_title, ch_sentence, ch_anti_sentence, ch_open, ch_prefix, ch_anti_prefix,\\\n",
    "                        seq, alt, p_alt, ELSE, W, D,\\\n",
    "                        warning, debug_pp, reset_globals\n",
    "import classes\n",
    "from classes import StC, StNum, StNoun, StVerb, I\n",
    "import ru_dictionary\n",
    "from ru_dictionary import ruwords, CW, add_runoun2, add_skl2, make_skl2\n",
    "import en_dictionary\n",
    "from en_dictionary import dict_adj, dict_noun, dict_pronoun_ip, dict_pronoun_dp, \\\n",
    "                        dict_numeral, dict_verb_simple, dict_verb_komu, r_adj_noun, dict_other,\\\n",
    "                        add_ennoun2, add_ennoun1\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.343813Z",
     "start_time": "2019-04-16T20:05:50.167811Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    parse_system = reload(parse_system)\n",
    "    S            = parse_system.S\n",
    "    SAttrs       = parse_system.SAttrs\n",
    "    ParseInfo    = parse_system.ParseInfo\n",
    "    tokenizer    = parse_system.tokenizer\n",
    "    tokenize     = parse_system.tokenize\n",
    "    ch_title     = parse_system.ch_title\n",
    "    ch_sentence  = parse_system.ch_sentence\n",
    "    ch_anti_sentence = parse_system.ch_anti_sentence\n",
    "    ch_open      = parse_system.ch_open\n",
    "    ch_prefix    = parse_system.ch_prefix\n",
    "    ch_anti_prefix = parse_system.ch_anti_prefix\n",
    "    seq          = parse_system.seq\n",
    "    alt          = parse_system.alt\n",
    "    p_alt        = parse_system.p_alt\n",
    "    ELSE         = parse_system.ELSE\n",
    "    W            = parse_system.W\n",
    "    D            = parse_system.D\n",
    "    warning      = parse_system.warning\n",
    "    debug_pp     = parse_system.debug_pp\n",
    "    reset_globals = parse_system.reset_globals\n",
    "if 0:\n",
    "    classes = reload(classes)\n",
    "    StC    = classes.StC\n",
    "    StNum  = classes.StNum\n",
    "    StNoun = classes.StNoun\n",
    "    StVerb = classes.StVerb\n",
    "    I      = classes.I\n",
    "if 0:\n",
    "    ru_dictionary = reload(ru_dictionary)\n",
    "    ruwords     = ru_dictionary.ruwords\n",
    "    CW          = ru_dictionary.CW\n",
    "    add_runoun2 = ru_dictionary.add_runoun2\n",
    "    add_skl2    = ru_dictionary.add_skl2\n",
    "    make_skl2   = ru_dictionary.make_skl2\n",
    "if 0:\n",
    "    en_dictionary = reload(en_dictionary)\n",
    "    dict_adj        = en_dictionary.dict_adj\n",
    "    dict_noun       = en_dictionary.dict_noun\n",
    "    dict_pronoun_ip = en_dictionary.dict_pronoun_ip\n",
    "    dict_pronoun_dp = en_dictionary.dict_pronoun_dp\n",
    "    dict_numeral    = en_dictionary.dict_numeral\n",
    "    dict_verb_simple= en_dictionary.dict_verb_simple\n",
    "    dict_verb_komu  = en_dictionary.dict_verb_komu\n",
    "    r_adj_noun      = en_dictionary.r_adj_noun\n",
    "    dict_other      = en_dictionary.dict_other\n",
    "    add_ennoun2     = en_dictionary.add_ennoun2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Паттерны и правила: Составные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-Когда в правилах использовать S а когда один из классов Struct ?\n",
    "\n",
    "-S используется для неизменяемых узлов-листьев. Во всех остальных случаях используется один из классов Struct\n",
    "\n",
    "pull_attrs - если имеющуюся структуру надо расширить константами и распространить attrs структуры на расширенную структуру - смотри пример в как в r_U_noun_EST_noun\n",
    "значение pull_attrs задается равным исходной структуры в расширенной\n",
    "\n",
    "attrs_from_left/attrs_from_right - если в правиле один из аргументов пропадает, то его атрибуты желательно добавить к другому аргументу. \n",
    "Синтаксис `I(tag=remain_arg, ... , attrs_from_left=lost_arg)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Исключения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.383813Z",
     "start_time": "2019-04-16T20:05:50.363816Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_numeral(s,p):\n",
    "    return D(dict_numeral)(s,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.543820Z",
     "start_time": "2019-04-16T20:05:50.395814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#2->\n",
    "@debug_pp\n",
    "def p_adj(s,p):\n",
    "    return D(dict_adj)(s,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Noun-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.695825Z",
     "start_time": "2019-04-16T20:05:50.551822Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_adj_noun3(s,p): return p_alt(s,p,\n",
    "    seq([ alt(W('an'),W('a')), p_noun3 ],r_A_noun),\n",
    "    seq([ W('the')           , p_noun3 ],r_THE_noun),\n",
    "    seq([ W('good'), W('morning') ],r_GOOD_MORNING),             \n",
    "ELSE,\n",
    "    seq([ p_adj, p_noun3 ],r_adj_noun)\n",
    ")\n",
    "# r_adj_noun определен в en_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.723823Z",
     "start_time": "2019-04-16T20:05:50.703826Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# исключения\n",
    "def r_A_noun(_a,_n): return StNoun([\n",
    "    I(maindep=_n,         attrs_from_left=_a)\n",
    "])\n",
    "def r_THE_noun(_a,_n): return StNoun([\n",
    "    I(maindep=_n,         attrs_from_left=_a)\n",
    "])\n",
    "\n",
    "def r_GOOD_MORNING(_g,_m):  return r_adj_noun(\n",
    "    CW('добрый',_g),\n",
    "    CW('утро',_m)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:50.907833Z",
     "start_time": "2019-04-16T20:05:50.731827Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun3(s,p): return p_alt(s,p,\n",
    "    p_adj_noun3, #ELSE, # переход к следующему уровню\n",
    "    p_numeral,\n",
    "    D(dict_noun)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:51.175838Z",
     "start_time": "2019-04-16T20:05:50.923833Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun2_1(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun3, p_dops ], r_noun_dops), #ELSE, # переход к следующему уровню\n",
    "    p_noun3\n",
    ")\n",
    "def r_noun_dops(n,d): return StNoun([\n",
    "    I(maindep=n),\n",
    "    I(nodep=d)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:51.403844Z",
     "start_time": "2019-04-16T20:05:51.191840Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun2(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun2_1, p_numeral ], r_noun_numeral), #ELSE, # переход к следующему уровню\n",
    "    p_noun2_1\n",
    ")\n",
    "@debug_pp\n",
    "def p_dop_noun2(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun3, p_numeral ], r_noun_numeral), #ELSE, # переход к следующему уровню\n",
    "    p_noun3\n",
    ")\n",
    "def r_noun_numeral(n,num): return StNoun([\n",
    "    I(maindep=n),\n",
    "    I(nomer=num)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:51.563846Z",
     "start_time": "2019-04-16T20:05:51.415844Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun1(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_noun2,\n",
    "    D(dict_pronoun_dp)\n",
    ")\n",
    "@debug_pp\n",
    "def p_dop_noun1(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_dop_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_dop_noun2,\n",
    "    D(dict_pronoun_dp)\n",
    ")\n",
    "@debug_pp\n",
    "def p_noun1_ip(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_noun2,\n",
    "    D(dict_pronoun_ip)\n",
    ")\n",
    "def r_numeral_noun(num,n):\n",
    "    if num.chis!=n.chis :\n",
    "        warning('не совпадают числа числ. и сущ.:'+str(num)+str(n))\n",
    "    return StNum([\n",
    "        I(quantity=num,            chis=n.chis, rod=n.rod, odush=n.odush ),\n",
    "        I(maindep=n)\n",
    "    ],quantity=num.quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:51.799860Z",
     "start_time": "2019-04-16T20:05:51.583845Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun1, W('and'), p_noun ],r_noun_and_noun  ),\n",
    "        seq([ p_noun1, W(',')  , p_noun ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_noun1\n",
    "    )\n",
    "@debug_pp\n",
    "def p_dop_noun(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_dop_noun1, W('and'), p_dop_noun ],r_noun_and_noun  ),\n",
    "        seq([ p_dop_noun1, W(',')  , p_dop_noun ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_dop_noun1\n",
    "    )\n",
    "@debug_pp\n",
    "def p_noun_ip(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun1_ip, W('and'), p_noun_ip ],r_noun_and_noun  ),\n",
    "        seq([ p_noun1_ip, W(',')  , p_noun_ip ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_noun1_ip\n",
    "    )\n",
    "def r_noun_and_noun(sn,a,n):    return StNoun([\n",
    "    I(dep=sn),\n",
    "    I(nodep=S('и',a.attrs)),\n",
    "    I(dep=n)\n",
    "],c='mn', p='ip',o=False,r='m')\n",
    "def r_noun_comma_noun(sn,c,n):    return StNoun([\n",
    "    I(dep=sn),\n",
    "    I(punct=S(',',c.attrs)),\n",
    "    I(dep=n)\n",
    "],c='mn', p='ip',o=False,r='m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Существительные в разных формах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:52.120791Z",
     "start_time": "2019-04-16T20:05:51.843858Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun_dp(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_pronoun_dp)],r_noun_dp), \n",
    "    seq([ W('to'), p_noun ],r_TO_noun_dp)\n",
    ")\n",
    "def r_noun_dp(_n): return StNoun([\n",
    "    I(maindep=_n,         pad='dp')\n",
    "])\n",
    "\n",
    "def r_TO_noun_dp(_t,_n): return StNoun([\n",
    "    I(maindep=_n,         pad='dp', attrs_from_left=_t)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:52.462098Z",
     "start_time": "2019-04-16T20:05:52.136419Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# одно дополнение\n",
    "@debug_pp\n",
    "def p_where(s,p): return p_alt(s,p,\n",
    "    seq([W('in'), p_dop_noun ],r_V_noun_pp),\n",
    "    seq([W('on'), p_dop_noun ],r_NA_noun_pp),\n",
    ")\n",
    "def r_V_noun_pp(v,_n): return StC([\n",
    "    I(nodep=S('в',v.attrs)),\n",
    "    I(nodep=_n,  pad='pp'),\n",
    "])\n",
    "def r_NA_noun_pp(v,_n): return StC([\n",
    "    I(nodep=S('на',v.attrs)),\n",
    "    I(nodep=_n,  pad='pp'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:52.993252Z",
     "start_time": "2019-04-16T20:05:52.462098Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# одно дополнение\n",
    "@debug_pp\n",
    "def p_dop(s,p): return p_alt(s,p,\n",
    "    seq([W('to'), p_dop_noun ],r_X_noun_dp),\n",
    "    seq([W('from'),p_dop_noun],r_IZ_noun),\n",
    "    seq([W('with'),p_dop_noun],r_S_noun_tp),\n",
    "    p_where\n",
    ")\n",
    "def r_X_noun_dp(x,n): return StNoun([\n",
    "    I(maindep=n,  pad='dp',         attrs_from_left=x)\n",
    "])\n",
    "def r_IZ_noun(iz,n): return StNoun([\n",
    "    I(nodep=S('из',iz.attrs)),\n",
    "    I(maindep=n,  pad='rp')\n",
    "])\n",
    "def r_S_noun_tp(s,n): return StNoun([\n",
    "    I(nodep=S('с',s.attrs)),\n",
    "    I(maindep=n,  pad='tp')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:53.420573Z",
     "start_time": "2019-04-16T20:05:52.993252Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# последовательность дополнений\n",
    "@debug_pp\n",
    "def p_dops(s,p): return p_alt(s,p,\n",
    "    p_dop,\n",
    "    seq([p_dop, p_dops], r_seq_dops)\n",
    ")\n",
    "def r_seq_dops(d1,d2): return StC([\n",
    "    I(nodep=d1),\n",
    "    I(nodep=d2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## have/has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:53.749973Z",
     "start_time": "2019-04-16T20:05:53.432577Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_have_question(s,p):\n",
    "    p_have = alt(W('have'),W('has'))\n",
    "    return p_alt(s,p,\n",
    "        seq([p_have,p_noun_ip,p_noun],r_have_question),\n",
    "        seq([W('how'), W('many'), p_noun, p_have, p_noun_ip], [2,r_SKOLKO_noun_U_noun,r_SKOLKO_U_noun_noun])\n",
    "    )\n",
    "\n",
    "def r_have_question(_h,_n1,_n2):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h.attrs)),\n",
    "    I(nodep=_n2)\n",
    "])\n",
    "def r_SKOLKO_noun_U_noun(how,many,_n1,have,_n2):  return StC([\n",
    "    I(nodep=S('сколько',how.attrs), attrs_from_right=many),\n",
    "    I(nodep=_n1, pad='rp'),\n",
    "    I(nodep=S('у')),\n",
    "    I(nodep=_n2,   pad='rp', npad='n' )# у Него\n",
    "])\n",
    "def r_SKOLKO_U_noun_noun(how,many,_n1,have,_n2):  return StC([\n",
    "    I(nodep=S('сколько',how.attrs), attrs_from_right=many),\n",
    "    I(nodep=S('у')),\n",
    "    I(nodep=_n2,   pad='rp', npad='n' ),# у Него\n",
    "    I(nodep=_n1, pad='rp'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:53.985479Z",
     "start_time": "2019-04-16T20:05:53.757978Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_HAVE_noun(s,p):\n",
    "    p_HAVE_HAS = alt( W('have'), W('has') )\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun_ip, p_HAVE_HAS,          p_noun ],r_U_noun_EST_noun),\n",
    "        seq([ p_noun_ip, p_HAVE_HAS, W('no'), p_noun ],r_U_noun_NET_noun)\n",
    "#        seq([ p_noun, p_HAVE_HAS,          p_pronoun_dp ],r_noun_EST_U_noun),\n",
    "#        seq([ p_noun, p_HAVE_HAS, W('no'), p_pronoun_dp ],r_noun_NET_U_noun)\n",
    "    )\n",
    "def r_U_noun_EST_noun(_n1_,_h_,_n2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h_.attrs)),\n",
    "    I(nodep=_n2_)\n",
    "])\n",
    "\n",
    "def r_U_noun_NET_noun(_n1_,_h_,_no_,_n2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('нет',_h_.attrs)),\n",
    "    I(nodep=_n2_,        pad='rp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:54.139678Z",
     "start_time": "2019-04-16T20:05:54.000908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "px_HAVE_HAS = alt( W('have'), W('has') )\n",
    "@debug_pp\n",
    "def pe_noun_HAVE(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun_ip, px_HAVE_HAS          ],r_U_noun_EST),\n",
    "        seq([ p_noun_ip, px_HAVE_HAS, alt(W('no'),W('not')) ],r_U_noun_NET)\n",
    "#        seq([ p_noun, p_HAVE_HAS,          p_pronoun_dp ],r_noun_EST_U_noun),\n",
    "#        seq([ p_noun, p_HAVE_HAS, W('no'), p_pronoun_dp ],r_noun_NET_U_noun)\n",
    "    )\n",
    "def r_U_noun_EST(_n1_,_h_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h_.attrs))\n",
    "])\n",
    "\n",
    "def r_U_noun_NET(_n1_,_h_,_no_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('нет',_h_.attrs))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:54.346523Z",
     "start_time": "2019-04-16T20:05:54.155268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_HAVE(s,p): \n",
    "    p_have = alt(W('have'),W('has'))\n",
    "    return p_alt(s,p,\n",
    "        seq([p_have,          p_noun], r_IMET_noun_vp),\n",
    "        seq([p_have, W('no'), p_noun], r_NE_IMET_noun_vp),\n",
    "        seq([p_have],r_IMET),\n",
    "        seq([p_have,alt(W('no'),W('not'))],r_NE_IMET)\n",
    "    )\n",
    "def r_NE_IMET_noun_vp(_v,no,_n): return StVerb([\n",
    "    I(nodep=S('не',no.attrs)),\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "    I(vp=_n,   pad='vp')\n",
    "])\n",
    "def r_IMET_noun_vp(_v,_n): return StVerb([\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "    I(vp=_n,   pad='vp')\n",
    "])\n",
    "def r_IMET(_v): return StVerb([\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "])\n",
    "def r_NE_IMET(_v,no): return StVerb([\n",
    "    I(nodep=S('не',no.attrs)),\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## to_be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:54.508110Z",
     "start_time": "2019-04-16T20:05:54.361938Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_TOBE_where(s,p): return p_alt(s,p,\n",
    "    seq([p_noun_ip,p_TOBE,p_where],r_noun_X_where)\n",
    ")\n",
    "\n",
    "def r_noun_X_where(_n,x,_w): return StC([\n",
    "    I(nodep=_n),\n",
    "    I(nodep=_w, attrs_from_left=x)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:54.706192Z",
     "start_time": "2019-04-16T20:05:54.508110Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_TOBE_(s,p): return p_alt(s,p,\n",
    "    seq([W('be')],r_EST),\n",
    "    seq([W('am')],r_EST),\n",
    "    seq([W('is')],r_EST),\n",
    "    seq([W('are')],r_EST)\n",
    ")\n",
    "def r_EST(_v): return StVerb([\n",
    "    I(maindep=CW('есть (быть)',_v)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:54.857888Z",
     "start_time": "2019-04-16T20:05:54.721865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_TOBE(s,p): return p_alt(s,p,\n",
    "    seq([p_TOBE_, p_noun], \n",
    "          [1, r_EST_noun_ip, r_JAVLYATSA_noun_tp]),\n",
    "#    seq([p_tobe, W('no'), alt(p_noun,D(dict_pronoun_dp))], r_NE_IMET_noun_vp),\n",
    "    p_TOBE_\n",
    ")\n",
    "def r_EST_noun_ip(_v,_n): return StVerb([\n",
    "    I(maindep=CW('есть (быть)',_v)),\n",
    "    I(ip=_n,   pad='ip')\n",
    "])\n",
    "def r_JAVLYATSA_noun_tp(_v,_n): return StVerb([\n",
    "    I(maindep=CW('являться',_v)),\n",
    "    I(tp=_n,   pad='tp')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Глагол с дополнениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.016011Z",
     "start_time": "2019-04-16T20:05:54.893581Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# разделяемые правила\n",
    "def r_verb_noun_vp(_v,_p): return StVerb([\n",
    "    I(maindep= _v),\n",
    "    I(vp=_p,   pad='vp')\n",
    "])\n",
    "def r_verb_noun_dp(_v,_p): return StVerb([\n",
    "    I(maindep= _v),\n",
    "    I(dp=_p,   pad='dp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.180018Z",
     "start_time": "2019-04-16T20:05:55.028017Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать кому-то\n",
    "@debug_pp\n",
    "def p_verb3_komu(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_verb_komu), p_noun_dp], r_verb_noun_dp),\n",
    "    D(dict_verb_komu)\n",
    ")\n",
    "# r_verb_noun_dp - разделяемое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.380023Z",
     "start_time": "2019-04-16T20:05:55.188020Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать кому-то что-то\n",
    "@debug_pp\n",
    "def p_verb3_komu_chto(s,p): return p_alt(s,p,\n",
    "    seq([p_verb3_komu,                D(dict_pronoun_dp)], r_verb_noun_dp),\n",
    "    ELSE,\n",
    "    seq([ p_verb3_komu,                 p_noun        ], r_verb_noun_vp),\n",
    "    seq([ p_verb3_komu, W(':'),         p_phrase      ], r_verb_c_phrase),\n",
    "    seq([ p_verb3_komu,         W('\"'), p_text, W('\"')], r_verb_q_text),\n",
    "    seq([ p_verb3_komu, W(':'), W('\"'), p_text, W('\"')], r_verb_c_q_text), \n",
    "    p_verb3_komu\n",
    ")\n",
    "# r_verb_noun_vp - разделяемое\n",
    "# r_verb_noun_dp - разделяемое\n",
    "def r_verb_c_phrase(_v,c,_p): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=c),\n",
    "    I(nodep=_p)\n",
    "])\n",
    "def r_verb_q_text(_v,q1,_p,q2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=q1, add_changers={ch_open}),\n",
    "    I(nodep=_p),\n",
    "    I(punct=q2),\n",
    "])\n",
    "def r_verb_c_q_text(_v,c,q1,_p,q2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=c),\n",
    "    I(punct=q1, add_changers={ch_open}),\n",
    "    I(nodep=_p),\n",
    "    I(punct=q2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.577362Z",
     "start_time": "2019-04-16T20:05:55.392023Z"
    },
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# глагол с дополнением\n",
    "@debug_pp\n",
    "def p_verb2(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_verb_simple),W('from'),p_noun,W('to'), p_noun],re_verb_OT_noun_DO_noun),\n",
    "    ELSE,\n",
    "    seq([D(dict_verb_simple), p_dops],r_verb_dops),\n",
    "    D(dict_verb_simple)\n",
    ")\n",
    "\n",
    "def r_verb_dops(_v,_d): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(nodep=_d)\n",
    "])\n",
    "def re_verb_OT_noun_DO_noun(_v,ot,_n1,do,_n2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(nodep=S('от',ot.attrs)),\n",
    "    I(nodep=_n1,  pad='rp'),\n",
    "    I(nodep=S('до',do.attrs)),\n",
    "    I(nodep=_n2,  pad='rp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.761456Z",
     "start_time": "2019-04-16T20:05:55.608611Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать что-то\n",
    "@debug_pp\n",
    "def p_verb3_simple(s,p): return p_alt(s,p,\n",
    "    seq([p_verb2, p_noun], r_verb_noun_vp),\n",
    "    p_verb2\n",
    ")\n",
    "# r_verb_noun_vp - разделяемое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:55.941478Z",
     "start_time": "2019-04-16T20:05:55.777085Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать (кому-то что-то)\n",
    "@debug_pp\n",
    "def p_verb3_1(s,p): return p_alt(s,p,\n",
    "    p_verb3_simple,\n",
    "    p_verb3_komu_chto,\n",
    "    p_HAVE,\n",
    "    p_TOBE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:14.982275Z",
     "start_time": "2019-04-16T20:07:14.962274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# могу сделать\n",
    "@debug_pp\n",
    "def p_verb3(s,p): return p_alt(s,p,\n",
    "    seq([W('can'), p_verb3_1],r_CAN_verb),\n",
    "    p_verb3_1\n",
    ")\n",
    "def r_CAN_verb(c,v): return StVerb([\n",
    "    I(maindep=CW('мочь',c)),\n",
    "    I(nodep=v,   form='neopr')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Глагол(ы) с подлежащим, или другой формы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:20.056353Z",
     "start_time": "2019-04-16T20:07:20.028354Z"
    },
    "code_folding": [
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# некто делает\n",
    "@debug_pp\n",
    "def p_noun_verb1(s,p): return p_alt(s,p,\n",
    "    pe_noun_HAVE_noun,\n",
    "    pe_noun_HAVE,\n",
    "ELSE,\n",
    "    pe_noun_TOBE_where,\n",
    "    seq([ p_noun_ip , p_verb3 ],r_noun_verb),\n",
    "    seq([ p_noun_ip, p_verb3, W('too') ],r_noun_TOZHE_verb), #ELSE, # переход к следующему уровню\n",
    ")\n",
    "\n",
    "def r_noun_verb(n,v): return StVerb([\n",
    "    I(ip=n),\n",
    "    I(main=v,   form='nast', pers=n.pers, chis=n.chis, rod=n.rod)\n",
    "])\n",
    "def r_noun_TOZHE_verb(_n, _v, _t): return StVerb([\n",
    "    I(ip=_n),\n",
    "    I(nodep=S('тоже',_t.attrs)),\n",
    "    I(main=_v,   form='nast', pers=_n.pers, chis=_n.chis, rod=_n.rod)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:56.444546Z",
     "start_time": "2019-04-16T20:05:56.263709Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# некто делает/ делать/ делай\n",
    "@debug_pp\n",
    "def p_verb1(s,p): return p_alt(s,p,\n",
    "    p_noun_verb1,\n",
    "    seq([ W('to'), p_verb3 ],r_to_verb),   #ELSE, # переход к следующему уровню\n",
    "    seq([ p_verb3 ],[1,r_povel_verb_ed,r_povel_verb_mn])\n",
    ")\n",
    "def r_to_verb(_t,_v): return StVerb([\n",
    "    I(maindep=_v,         form='neopr', attrs_from_left=_t)\n",
    "])\n",
    "\n",
    "def r_povel_verb_ed(_v): return StVerb([\n",
    "    I(maindep=_v, asp='sov', form='povel',chis='ed') #\n",
    "])\n",
    "def r_povel_verb_mn(_v): return StVerb([\n",
    "    I(maindep=_v, asp='sov', form='povel',chis='mn') # asp='sov',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:56.627121Z",
     "start_time": "2019-04-16T20:05:56.459966Z"
    },
    "code_folding": [
     10,
     17
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать одно и/но сделать сдругое\n",
    "@debug_pp\n",
    "def p_verb(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun_ip, px_HAVE_HAS, p_noun1, W(',')  , p_noun,  W('and'), p_verb1 ],re_U_noun_EST_noun_C_noun_I_verb),\n",
    "    ELSE,\n",
    "    seq([ p_verb1, W(','), p_verb1 ]          ,r_verb_c_verb),   \n",
    "    seq([ p_verb1, W(','), W('but'), p_verb1 ],r_verb_NO_verb),   \n",
    "    seq([ p_verb1, W('and'), p_verb1 ]        ,r_verb_I_verb),\n",
    "    #ELSE, # переход к следующему уровню\n",
    "    p_verb1\n",
    ")\n",
    "\n",
    "def r_verb_NO_verb(_v1_,_c_,_but_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=_c_),\n",
    "    I(nodep=S('но',_c_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def r_verb_c_verb(_v1_,_c_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=_c_),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def r_verb_I_verb(_v1_,_i_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=S('и',_i_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def re_U_noun_EST_noun_C_noun_I_verb(_n1_,_h_,sn,c,n,_i_,_v2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=StC([\n",
    "            I(nodep=S('у')),\n",
    "            I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "        ]), pull_attrs=1 ),\n",
    "        I(nodep=S('есть',_h_.attrs)),\n",
    "        I(nodep=StNoun([\n",
    "            I(dep=sn),\n",
    "            I(punct=S(',',c.attrs)),\n",
    "            I(dep=n)\n",
    "        ],c='mn', p='ip',o=False,r='m'))\n",
    "    ])),\n",
    "    I(nodep=S('и',_i_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Фразы, предложения, текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:56.808819Z",
     "start_time": "2019-04-16T20:05:56.634983Z"
    },
    "code_folding": [
     1,
     14,
     20,
     26
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_phrase(s,p): \n",
    "    return p_alt(s,p,\n",
    "        p_verb,    #ELSE,\n",
    "        p_noun_ip,    #ELSE,\n",
    "        p_noun_dp, #ELSE,\n",
    "        p_dop,\n",
    "        p_adj,\n",
    "        D(dict_other),\n",
    "        seq([W('yes'),W(','),p_verb],r_DA_COMMA_verb),\n",
    "        seq([W('no') ,W(','),p_verb],r_NET_COMMA_verb),\n",
    "        seq([p_noun  ,W(','),p_verb],r_noun_COMMA_verb),\n",
    "    )\n",
    "\n",
    "def r_DA_COMMA_verb(yes,comma,_v):    return StC([\n",
    "    I(nodep=S('да',yes.attrs)),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])\n",
    "\n",
    "def r_NET_COMMA_verb(no,comma,_v):    return StC([\n",
    "    I(nodep=S('нет',no.attrs)),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])\n",
    "\n",
    "def r_noun_COMMA_verb(_n,comma,_v):    return StC([\n",
    "    # обращение\n",
    "    I(nodep=_n),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:56.965913Z",
     "start_time": "2019-04-16T20:05:56.824129Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_question_phrase(s,p): \n",
    "    return p_have_question(s,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:57.190213Z",
     "start_time": "2019-04-16T20:05:56.965913Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_proper={}# имена собственные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:57.377204Z",
     "start_time": "2019-04-16T20:05:57.205659Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_sentence(s,p):\n",
    "    first_capital = s[p]=='I' or ch_title in s[p].attrs.changers\n",
    "    def r_sentence(ph,d):\n",
    "        rez=StC([I(nodep=ph),I(punct=d)])\n",
    "        if first_capital: rez.attrs.changers|={ch_sentence}\n",
    "        rez.attrs.pre = prefix + rez.attrs.pre\n",
    "        rez.attrs.changers|={ch_prefix}\n",
    "        return rez\n",
    "    restore_title=False\n",
    "    if ch_title in s[p].attrs.changers and s[p] not in dict_proper:\n",
    "        s[p].attrs.changers-={ch_title}\n",
    "        s[p].attrs.changers|={ch_anti_sentence}# хак, реализованный в SAttrs.join()\n",
    "        restore_title=True\n",
    "    prefix = s[p].attrs.pre\n",
    "    s[p].attrs.changers |= {ch_anti_prefix}\n",
    "    try:\n",
    "        rezs=p_alt(s,p,\n",
    "            seq([p_phrase,alt(W('.'),W('!'))],r_sentence),\n",
    "            seq([p_question_phrase,W('?')],r_sentence)\n",
    "        )\n",
    "    finally:# не работает, т.к. всё закешировалось\n",
    "        if restore_title:\n",
    "            s[p].attrs.changers|={ch_title}\n",
    "            s[p].attrs.changers-={ch_anti_sentence}\n",
    "        s[p].attrs.pre = prefix\n",
    "    return rezs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:57.556698Z",
     "start_time": "2019-04-16T20:05:57.392836Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_text(s,p):\n",
    "    '''или последовательность предложений или 1 фраза\n",
    "    p_text::= p_sentence* | p_phrase\n",
    "    '''\n",
    "    rez=[]\n",
    "    while p<len(s):\n",
    "        rezs=maxlen_filter(p_sentence,s,p)\n",
    "        if len(rezs)==0: break\n",
    "        p1,r1=rezs[0] # отбрасываем остальные результаты\n",
    "        p=p1\n",
    "        rez.append(I(nodep=r1))\n",
    "    if len(rez)>0:\n",
    "        return [(p,StC(rez))]\n",
    "    else:\n",
    "        return maxlen_filter(alt(p_phrase,p_question_phrase),s,p)\n",
    "\n",
    "def maxlen_filter(patt,s,p):\n",
    "    '''находит самые длинные результаты, а остальные отбрасывает\n",
    "    \n",
    "    если самых длинных несколько - warning\n",
    "    '''\n",
    "    rezs=patt(s,p)\n",
    "    m=0\n",
    "    im=set()\n",
    "    for i in range(len(rezs)):\n",
    "        if rezs[i][0]>m:\n",
    "            m=rezs[i][0]\n",
    "            im={i}\n",
    "        elif rezs[i][0]==m:\n",
    "            im.add(i)\n",
    "    long_rezs= [rezs[i] for i in im]\n",
    "    if len(long_rezs)>1:\n",
    "        #print(p,m,s[p:m],SAttrs().join(s))\n",
    "        warning('multiple results:\\n'+\n",
    "            SAttrs().join(s[p:m])+'\\n'+\n",
    "            '\\n'.join(r.tostr() for void,r in long_rezs)\n",
    "        )\n",
    "            \n",
    "    return [] if len(long_rezs)==0 else long_rezs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Запуск и отладка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:57.790901Z",
     "start_time": "2019-04-16T20:05:57.587948Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _en2ru(s): # main\n",
    "    ''' (text|.)* + warning-и\n",
    "    '''\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    if len(s)==0:\n",
    "        warning('no tokens')\n",
    "        return ''\n",
    "    \n",
    "    ret_s = ''\n",
    "    p=0\n",
    "    while p<len(s):\n",
    "        #print('ITERATION',p)\n",
    "        rezs= p_text(s,p)\n",
    "        if len(rezs)==0:\n",
    "            warning(\"CAN'T TRANSLATE: \"+s[p])\n",
    "            ret_s += (' | ' if p>0 else '')+ s[p]\n",
    "            p+=1\n",
    "        else:\n",
    "            p1,r1 = rezs[0] # отбрасываем остальные результаты\n",
    "            #print(p,p1,r1)\n",
    "            s1 = r1.tostr()\n",
    "            #print(p,p1,r1)\n",
    "            ret_s += (' | ' if p>0 else '')+ s1\n",
    "            if p>0:\n",
    "                warning('TRANSLATION BREAKS')\n",
    "            assert p1>p, rezs\n",
    "            p=p1\n",
    "    return ret_s\n",
    "\n",
    "def en2ru(s):\n",
    "    '''переводит строку, возвращает строку'''\n",
    "    parse_system.DEBUGGING=False\n",
    "    return _en2ru(s)\n",
    "\n",
    "def d_en2ru(s):\n",
    "    '''переводит строку, возвращает строку\n",
    "    \n",
    "    дополнительно пишет отладочный вывод'''\n",
    "    l_d = parse_system.DEBUGGING\n",
    "    parse_system.DEBUGGING=True\n",
    "    try:\n",
    "        r=_en2ru(s)\n",
    "    finally:\n",
    "        parse_system.DEBUGGING=l_d\n",
    "    return r\n",
    "\n",
    "def pr_l_repr(s):\n",
    "    \"\"\"печатает строку в тройных кавычках\"\"\"\n",
    "    print(\"'''\"+s+\"'''\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:57.963302Z",
     "start_time": "2019-04-16T20:05:57.806315Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decline(s,pads=['ip','rp','dp','vp','tp','pp']):\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    # добавить дочитывание точки и остаточных пробелов\n",
    "    rezs=[res for pos,res in p_noun(s,0) if pos==len(s)]\n",
    "    if len(rezs)!=1:\n",
    "        raise TextError(rezs)\n",
    "    tmp=rezs[0]\n",
    "    \n",
    "    m=[]\n",
    "    for p in pads:\n",
    "        #print(str(tmp))\n",
    "        prompt= \\\n",
    "            '' if p=='ip' else\\\n",
    "            'нет ' if p=='rp' else\\\n",
    "            'дать ' if p=='dp' else\\\n",
    "            'вижу ' if p=='vp' else\\\n",
    "            'творю ' if p=='tp' else\\\n",
    "            'думаю о ' if p=='pp' else\\\n",
    "            throw(ValueError('bad pad: '+p))\n",
    "        #rez=deepcopy(tmp)\n",
    "        tmp.pad=p\n",
    "        m.append(prompt+tmp.tostr())#        print(prompt+str(tmp))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:58.129164Z",
     "start_time": "2019-04-16T20:05:57.978926Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _parse_pat(patt,s):\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    return patt(s,0)\n",
    "\n",
    "def parse_pat(patt,s):\n",
    "    '''парсит строку паттерном, и возвращает не стрингифицированный объект'''\n",
    "    parse_system.DEBUGGING=False\n",
    "    return _parse_pat(patt,s)\n",
    "\n",
    "def d_parse_pat(patt,s):\n",
    "    '''парсит строку паттерном, и возвращает не стрингифицированный объект\n",
    "    \n",
    "    дополнительно пишет отладочный вывод'''\n",
    "    l_d = parse_system.DEBUGGING\n",
    "    parse_system.DEBUGGING=True\n",
    "    try:\n",
    "        r=parsepat(s,patt)\n",
    "    finally:\n",
    "        parse_system.DEBUGGING=l_d\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:05:58.430894Z",
     "start_time": "2019-04-16T20:05:58.144796Z"
    },
    "code_folding": [
     21,
     57,
     117,
     137,
     156,
     187
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scheme(s,detailed=1):\n",
    "    '''печатает схему разбора\n",
    "    \n",
    "    есть дополнительный аргумент datailed\n",
    "        0 - не печатает сквозные паттерны (которые без правил)\n",
    "        1 - дефолтный вывод\n",
    "        2 - дополнительно печатает нестрингифицированные объекты\n",
    "    '''\n",
    "    # токенизируем строку\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    \n",
    "    # парсим строку\n",
    "    ParseInfo.enabled = True\n",
    "    try:\n",
    "        rezs=maxlen_filter(alt(p_sentence,p_phrase,p_question_phrase),s,0)\n",
    "    finally:\n",
    "        ParseInfo.enabled = False\n",
    "        \n",
    "    print(len(rezs),'результатов')\n",
    "    # анализируем каждый результат\n",
    "    for end,rez in rezs:\n",
    "        print()\n",
    "        # простой построчный вывод узлов результата\n",
    "        if detailed==2:\n",
    "            def print_rez0(rez,depth):\n",
    "                info = rez.parse_info\n",
    "                if hasattr(info,'p_start'):\n",
    "                    print('  '*depth+' '+SAttrs().join( s[info.p_start : info.p_end] ))\n",
    "                if hasattr(info,'patterns') or hasattr(info,'rule_group'):\n",
    "                    if hasattr(info,'patterns'):\n",
    "                        #'<'+str(id(info.patterns))+'>'+\n",
    "                        patterns = ' '.join(info.patterns) if detailed>=1 else info.patterns[0]\n",
    "                    else:\n",
    "                        patterns = ''\n",
    "                    if hasattr(info,'rule_group'):\n",
    "                        if type(info.rule_group)==list:\n",
    "                            n = info.rule_group[0] if info.rule_group[0]!=0 else 1\n",
    "                            rule = info.rule_group[n]\n",
    "                            rules = str(n)+'/'+str(len(info.rule_group)-1)+' '\n",
    "                        else:\n",
    "                            rule = info.rule_group\n",
    "                            rules = ''\n",
    "                        rules += (rule.__name__ if callable(rule) else str(rule))\n",
    "                    else:\n",
    "                        rules = ''\n",
    "                    print('  '*depth +' '+ patterns+' -> '+rules)\n",
    "                print('  '*depth+'*'+str(rez))\n",
    "                print('  '*depth+'*'+repr(rez))\n",
    "\n",
    "                if hasattr(rez,'talk'):\n",
    "                    for x in rez.talk:\n",
    "                        print_rez0(x[1],depth+1)\n",
    "        \n",
    "            rez.pull_deferred()\n",
    "            print_rez0(rez,0)\n",
    "        \n",
    "        # создание узлов\n",
    "        class Node:\n",
    "            __slots__ = ['childs','p_start','p_end','patterns','rule','rez']\n",
    "        def make_tree(struct):\n",
    "            if hasattr(struct,'talk'):\n",
    "                if hasattr(struct.parse_info,'p_start'):\n",
    "                    info = struct.parse_info\n",
    "                    node = Node()\n",
    "                    node.p_start = info.p_start\n",
    "                    node.p_end = info.p_end\n",
    "                    node.patterns = info.patterns\n",
    "                    if hasattr(info,'rule_group'):\n",
    "                        if type(info.rule_group)==list:\n",
    "                            assert info.rule_group[0]!=0, info.rule_group\n",
    "                            rule = info.rule_group[info.rule_group[0]]\n",
    "                            rules = str(info.rule_group[0])+'/'+str(len(info.rule_group)-1)+' '\n",
    "                        else:\n",
    "                            rule = info.rule_group\n",
    "                            rules = ''\n",
    "                        node.rule = rules + (rule.__name__ if callable(rule) else str(rule))\n",
    "                    else:\n",
    "                        node.rule = ''\n",
    "                    node.rez = struct\n",
    "                    node.childs = []\n",
    "                    depth = 0\n",
    "                    for tup in struct.talk:\n",
    "                        d,m = make_tree(tup[1])\n",
    "                        if d>depth: depth = d\n",
    "                        node.childs+=m\n",
    "                    node.childs.sort(key=lambda node:node.p_start)\n",
    "                    return (depth+1,[node])\n",
    "                else:\n",
    "                    mm = []\n",
    "                    depth = 0\n",
    "                    for tup in struct.talk:\n",
    "                        d,m = make_tree(tup[1])\n",
    "                        if d>depth: depth = d\n",
    "                        mm+=m\n",
    "                    return (depth,mm)\n",
    "            elif hasattr(struct.parse_info,'p_start'):\n",
    "                info = struct.parse_info\n",
    "                node = Node()\n",
    "                node.p_start = info.p_start\n",
    "                node.p_end = info.p_end\n",
    "                node.patterns = info.patterns\n",
    "                if hasattr(info,'rule_group'):\n",
    "                    if type(info.rule_group)==list:\n",
    "                        no = info.rule_group[0] if info.rule_group[0]!=0 else 1\n",
    "                        rule = info.rule_group[no]\n",
    "                        rules = str(info.rule_group[0])+'/'+str(len(info.rule_group)-1)+' '\n",
    "                    else:\n",
    "                        rule = info.rule_group\n",
    "                        rules = ''\n",
    "                    node.rule = rules + (rule.__name__ if callable(rule) else str(rule))\n",
    "                else:\n",
    "                    node.rule = ''\n",
    "                node.rez = struct\n",
    "                node.childs = []\n",
    "                return (1,[node])\n",
    "            else:\n",
    "                return (0,[])\n",
    "        \n",
    "        # простой вывод узлов\n",
    "        if 0:\n",
    "            def print_rez1(info,depth):\n",
    "                if hasattr(info,'p_start'):\n",
    "                    print('  '*depth+' '+SAttrs().join( s[info.p_start : info.p_end] ))\n",
    "                patterns = ' '.join(info.patterns) if full else info.patterns[0]\n",
    "                print('  '*depth +' '+ patterns+' -> '+info.rule)\n",
    "                print('  '*depth+'*'+str(info.rez))\n",
    "\n",
    "                if hasattr(info,'childs'):\n",
    "                    for x in info.childs:\n",
    "                        print_rez1(x,depth+1)\n",
    "                    \n",
    "            rez.pull_deferred()\n",
    "            \n",
    "        # создание дерева\n",
    "        depth,mm = make_tree(rez)\n",
    "        assert len(mm)==1\n",
    "        tree=mm[0]\n",
    "        \n",
    "        # простой вывод узлов\n",
    "        if 0:\n",
    "            print('depth=',depth)\n",
    "            print_rez1(tree,0)\n",
    "        \n",
    "        DISTANCE = 2\n",
    "        cols = []\n",
    "        pos=0\n",
    "        for word in s:\n",
    "            cols.append(pos)\n",
    "            pos+=len(word)+DISTANCE\n",
    "        cols.append(pos)\n",
    "        \n",
    "        class Line:\n",
    "            __slots__=['h','line']\n",
    "            def __init__(self,h,l):\n",
    "                self.h = h\n",
    "                self.line = l\n",
    "                \n",
    "        lines = [Line(0,[None for i in range(len(s))]) for j in range(depth)]\n",
    "        def make_lines(node):\n",
    "            dd=0\n",
    "            for c in node.childs:\n",
    "                d = make_lines(c)\n",
    "                if d>dd: dd = d\n",
    "                    \n",
    "            node.rez = repr(node.rez.tostr())[1:-1]\n",
    "            for i in range(1,len(node.patterns)):\n",
    "                node.patterns[i] = '('+node.patterns[i]+')'\n",
    "            \n",
    "            if len(node.patterns)>lines[dd].h: lines[dd].h = len(node.patterns)\n",
    "                \n",
    "            maxlen = max(len(node.rule),len(node.rez),max([len(i) for i in node.patterns]))\n",
    "            dlen = (maxlen+DISTANCE) - (cols[node.p_end]-cols[node.p_start])\n",
    "            #print(node.rez,maxlen,dlen)\n",
    "            if dlen>0:\n",
    "                for i in range(node.p_end,len(cols)):\n",
    "                    cols[i]+=dlen\n",
    "            \n",
    "            lines[dd].line[node.p_start] = node\n",
    "            return dd+1\n",
    "        dd = make_lines(tree)\n",
    "        assert dd==depth\n",
    "        \n",
    "        def inde(s,p_start,p_end):\n",
    "            return s+' '*(cols[p_end]-cols[p_start]-len(s))\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            print(inde(s[i],i,i+1),end='')\n",
    "        print()\n",
    "        \n",
    "        for line_ in lines:\n",
    "            line=line_.line\n",
    "            \n",
    "            # _______\n",
    "            i=0\n",
    "            l=''\n",
    "            while i<len(s):\n",
    "                if line[i]==None:\n",
    "                    l+=inde('',i,i+1)\n",
    "                    i+=1\n",
    "                else:\n",
    "                    l+='_'*(cols[line[i].p_end]-cols[line[i].p_start]-DISTANCE)+' '*DISTANCE\n",
    "                    i=line[i].p_end\n",
    "            print(l)\n",
    "            \n",
    "            # patterns[0]\n",
    "            i=0\n",
    "            l=''\n",
    "            while i<len(s):\n",
    "                if line[i]==None:\n",
    "                    l+=inde('',i,i+1)\n",
    "                    i+=1\n",
    "                else:\n",
    "                    l+=inde(line[i].patterns[0],line[i].p_start,line[i].p_end)\n",
    "                    i=line[i].p_end\n",
    "            print(l)\n",
    "\n",
    "            # rule\n",
    "            i=0\n",
    "            l=''\n",
    "            while i<len(s):\n",
    "                if line[i]==None:\n",
    "                    l+=inde('',i,i+1)\n",
    "                    i+=1\n",
    "                else:\n",
    "                    l+=inde(line[i].rule,line[i].p_start,line[i].p_end)\n",
    "                    i=line[i].p_end\n",
    "            print(l)\n",
    "\n",
    "            # rez\n",
    "            i=0\n",
    "            l=''\n",
    "            while i<len(s):\n",
    "                if line[i]==None:\n",
    "                    l+=inde('',i,i+1)\n",
    "                    i+=1\n",
    "                else:\n",
    "                    l+=inde(line[i].rez,line[i].p_start,line[i].p_end)\n",
    "                    i=line[i].p_end\n",
    "            print(l)\n",
    "            \n",
    "            if detailed>=1:\n",
    "                for j in range(1,line_.h):\n",
    "                    # patterns[j]\n",
    "                    i=0\n",
    "                    l=''\n",
    "                    while i<len(s):\n",
    "                        if line[i]==None or j>=len(line[i].patterns):\n",
    "                            l+=inde('',i,i+1)\n",
    "                            i+=1\n",
    "                        else:\n",
    "                            l+=inde(line[i].patterns[j],line[i].p_start,line[i].p_end)\n",
    "                            i=line[i].p_end\n",
    "                    print(l)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тесты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "    КОДИТЬ И ДЕБАЖИТЬ ТЕКУЩЕЕ\n",
    "    12) прилагательные, расположение\n",
    "    13) где?\n",
    "    14) какого цвета\n",
    "        КОНТЕКСТ\n",
    "    15) посмотри\n",
    "\n",
    "автовыбор склонений/спряжений для существительных с одним числом, прилагательных и глаголов\n",
    "регламентировать использование attrs-ов в правилах и только потом приступать к контекстам\n",
    "сделать устранение конфликтов исключений\n",
    "\n",
    "на потом: \n",
    "\tзадание глобального контекста (задание дефолтных правил)\n",
    "\tвыбор паттернов и правил в зависимости от времени \n",
    "\n",
    "работа с деревом вглубь:\n",
    "\tпросмотр вглубь возможен\n",
    "\t\n",
    "\tу каждого узла ссылка на правило и его аргументы\n",
    "\tв узлах дерева поля \n",
    "\t\tcontext_dep\n",
    "\t\t\tTrue - узел зависит от контекста\n",
    "\t\t\t\tссылка на правило, также принимат контекст\n",
    "\t\t\tFalse - узел не зависит от контекста\n",
    "\t\tcontect_dep_srcs - массив номеров - \n",
    "\t\t\tкакие аргументы правила зависят от контекста (или их потомки зависят от контекста)\n",
    "\t\t\tт.е. какие аргументы правила требую ремейка в случае изменения контекста\n",
    "\t\t\t\n",
    "\t\tcontext(может отсутствовать) - словарь (строка, ссылка на узел), который является контекстом\n",
    "\t\t\t- устанавливается в правилах\n",
    "\tфункция context_remake(node,context)\n",
    "\n",
    "два рыжих кота\n",
    "вижу двух рыжих котов\n",
    "вижу два горячих утюга\n",
    "два пирожных\n",
    "\n",
    "watch, двое, трое, пятеро\n",
    "\n",
    "написать везде строки документации\n",
    "написать инструкцию как пользоваться\n",
    "\n",
    "scheme('it')\n",
    "\n",
    "...\n",
    "открывающиеся кавычки\n",
    "\n",
    "для больших текстов p_sentence будет делать срез со своей позции до конца\n",
    "    - чтобы обновить кэши ф-ций\n",
    "\n",
    "исключения парсить, если регулярным образом распарсилось\n",
    "    каждая функция будет с аргументом: парсить или нет исключения\n",
    "\n",
    "атрибуты слов: (теги)\n",
    "отображение открывающейся кавычки (SAttrs.join)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:16:04.392128Z",
     "start_time": "2019-04-17T08:15:53.269046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook en2ru.ipynb to script\n",
      "[NbConvertApp] Writing 42060 bytes to en2ru.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script en2ru.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:28.651450Z",
     "start_time": "2019-04-16T20:07:28.623456Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "en2ru('I see jam and one cup.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:29.837388Z",
     "start_time": "2019-04-16T20:07:29.825384Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decline('two watches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:31.137421Z",
     "start_time": "2019-04-16T20:07:31.061416Z"
    }
   },
   "outputs": [],
   "source": [
    "# you - ты/вы\n",
    "pr_l_repr(en2ru('''Have you a cat? Yes,\n",
    "we have.\n",
    "How many kittens has\n",
    "the cat?\n",
    "It has one kitten.\n",
    "How many ducks have\n",
    "you?\n",
    "We have two ducks and\n",
    "ten ducklings.'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-16T20:07:32.813730Z",
     "start_time": "2019-04-16T20:07:32.709729Z"
    }
   },
   "outputs": [],
   "source": [
    "# контексты!!!\n",
    "pr_l_repr(en2ru('''How many chickens has the hen?\n",
    "It has eleven.\n",
    "How many ducklings has the duck?\n",
    "It has eight.\n",
    "How many kittens has the cat?\n",
    "It has three.\n",
    "How many dolls has the girl?\n",
    "She has two.\n",
    "How many sticks has the boy?\n",
    "He has five.\n",
    "How many hats have I?\n",
    "You have one.\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T08:13:22.610503Z",
     "start_time": "2019-04-17T08:13:19.905515Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tests\n",
    "tests = reload(tests)\n",
    "tests.init(parse_system,parse_system.TestError,seq,W,SAttrs,\n",
    "           tokenize,\n",
    "           en2ru,decline,scheme,d_en2ru,pr_l_repr,p_noun,p_noun1,r_noun_comma_noun,\n",
    "          1,False)\n",
    "tests.test1()\n",
    "tests.test2()\n",
    "tests.test3()\n",
    "tests.test4()\n",
    "tests.test5and6()\n",
    "tests.test7()\n",
    "tests.test8()\n",
    "tests.test8_1()\n",
    "tests.test9()\n",
    "tests.test10()\n",
    "tests.test11()\n",
    "tests.finalize()\n",
    "tests.TEST_ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
