{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Зачем всё это?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Есть машинный перевод и есть автоматизированный перевод. \n",
    "\n",
    "Машинный перевод дает 100% автоматизации и не позволяет вмешиваться в процесс перевода (только в результат). Конечно у многих машинных переводчиков есть функция \"предложить перевод\", но я не понимаю, как она работает, и работает ли хоть как-то вообще. Однажды я всё-таки заметил, как это работает: в яндекс-переводчике предложил перевод слова (уже не помню какого), и через сутки до него дошло, как надо переводить это слово. Через сутки, Карл!\n",
    "\n",
    "Автоматизированный перевод дает где-то 15-20% автоматизации. Помимо словарей, глоссариев и прочей справочной информации, самая продвинутая (известная мне) технология - это память переводов, когда человек вручную переводит предложения, а система запоминает эти переводы, и если встречается предложение, которое было переведено раньше (или _похожее_ на него), его перевод подставляется автоматически. Но какова вероятность встретить в тексте 2 одинаковых предложения, если в них больше трёх слов?\n",
    "\n",
    "Целью данного переводчика является автоматизация 90%. Не 100 и не 20. А также мгновенное вступление изменений в силу. Ну и возможность залезть в код.\n",
    "\n",
    "-----\n",
    "\n",
    "Любая система перевода состоит из двух аспектов:\n",
    "* используемые структуры данных и алгоритмы\n",
    "* способ заполнения базы данных\n",
    "\n",
    "Есть следующие способы заполнения базы данных:\n",
    "* статистический: у статистического перевода и у нейросетевого\n",
    "* ручной: у перевода основанного на пправилах и у памяти переводов\n",
    "\n",
    "Какими бы умными ни были нейросетевые системы, системы с ручным заполнением базы данных всегда будут оставаться актуальными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Основной принцип"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "p_паттерн парсит текст (str, pos), \n",
    "    вызывая другие паттерны, возвращающие древовидные структуры\n",
    "    эти древовидные структуры передает одному из правил, сопоставленных данному паттерну\n",
    "    и возвращает (pos, результат этого правила), помещенный в массив\n",
    "    \n",
    "    если ничего не удалось распарсить, возвращаемый массив будет пустым\n",
    "    если удалось распарсить несколько вариантов - в массиве будет насколько вариантов\n",
    "        сначала парсятся все обычные варианты\n",
    "        и если есть хоть один обычный результат, \n",
    "            то парсятся все исключения\n",
    "            в массиве результатов исключения замещают результаты, если их длины совпадают\n",
    "```\n",
    "\n",
    "```\n",
    "r_правило получает список древовидных структур\n",
    "    обрабатывает их по определенному правилу\n",
    "        т.е. меняет параметры аргументов\n",
    "    возвращает древовидную структуру\n",
    "        т.е. создает структуру, содержащую в себе аргументы\n",
    "    если в группе правил все правила отключены, то результатом будет 0 или ссылка на эту группу правил\n",
    "    отключать все правила допустимо только в исключениях\n",
    "```\n",
    "\n",
    "```\n",
    "древовидная структура - объект определенного класса, соответствующего части речи, который содержит\n",
    "    постоянные параметры (для сущ.: род, число)\n",
    "    переменные параметры (для сущ.: падеж)\n",
    "        при изменении этих параметров автоматически менются параметры дочерних древовидных структур\n",
    "    talk: массив древовидных структур\n",
    "        или пар (тип, структура), где тип - main/dep/other\n",
    "    и др.\n",
    "```\n",
    "\n",
    "```\n",
    "str(древовидная структура)\n",
    "    превращает древовидную структуру в строку\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Что с этим делать дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "В дальнейшем предполагается, что паттерны и правила будут писать пользователи\n",
    "грамматика не преобразовывается в LL(1) или какой-то другой промежуточный формат,\n",
    "а парсится как есть, с возвратами (LL(*)), поэтому результаты нетерминалов кэшируются.\n",
    "Также стоит защита от зацикливания.\n",
    "Никто не обещал, что грамматика будет однозначной, \n",
    "поэтому каждый нетерминал возвращает массив результатов.\n",
    "Но в конечном итоге таких результатов должно быть немного.\n",
    "Ситуация, когда получаются одинаковые результаты явлется нежелательной.\n",
    "\n",
    "Уточним терминологию:\n",
    "нетерминал - набор альтернатив паттернов\n",
    "паттерн - последовательность терминалов/нетерминалов\n",
    "\n",
    "В дальнейшем предполагается, что будет центральная грамматика, \n",
    "а у ее правил пользователи будут создавать исключения и расширения.\n",
    "Фишка в том, что при редактировании грамматики этим способом \n",
    "    поведение грамматики для уже имеющихся тестов/текстов не изменится.\n",
    "Возможно периодически для оптимизации грамматики будет требоваться полная ее переработка,\n",
    "но чисто математическая (т.е. не требующая тестов/текстов) и довольно вычислительно-сложная задача.\n",
    "Впрочем и без оптимизации производительность ухудшается не сильно.\n",
    "\n",
    "Паттерн A является исключением паттерна B, если \n",
    "всё что может разобрать паттерн A может разобрать паттерн B,\n",
    "т.е. A задает подъязык языка B, \n",
    "т.е. A является частным случаем B (но связано с другим правилом).\n",
    "Сначала парсится B, и если это оказалось удачным, парсится A.\n",
    "Если А распарсилось неудачно, то результатом станосится результат B,\n",
    "а если удачно - то результат B отбрасывается и результатом станосится результат A.\n",
    "\n",
    "т.к. исключения являются обычными нетерминалами, то внутри них тоже можно делать исключения\n",
    "\n",
    "Расширения просто добавляются в список альтернатив.\n",
    "Можно было бы просто в нетерминалы добавлять новые альтернативы, \n",
    "но это может привести к появлению разных вариантов разбора.\n",
    "В этих случаях можно было бы создавать исклчения, разрешающие неоднозначность,\n",
    "но чтобы всё происходило автоматически для уже переведенных текстов\n",
    "надо чтобы результат помечался датой, которая является максимумом \n",
    "    из дат результатов (которые разобрал паттерн-последовательность) и даты создания этого паттерна.\n",
    "Если в альтернативу попадают результаты одинаковой длины, то \n",
    "    новые отбрасываются и остается только старый.\n",
    "\n",
    "В дальнейшем предполагается возможность каждый паттерн связывать с \n",
    "набором правил, а точнее с одним правилом из заданного набора.\n",
    "А также возможность эти наборы пополнять и легко менять вариант перевода.\n",
    "Одним из правил перевода исключения будет вариант, когда\n",
    "результат исключения отбрасывается а результатом становится \n",
    "результат правила регулярного паттерна.\n",
    "Это дает возможность отключать исключения, т.к. они всё же вносят изменения в регулярный перевод.\n",
    "\n",
    "Семантически возникают разные варианты использования наборов правил:\n",
    "- смысловой\n",
    "    в этом случае как правило смысл паттерна не меняется на протяжении всего текста\n",
    "- указательный (контекстный): it, this, that, you\n",
    "    ...\n",
    "- эстетический\n",
    "    его решать лучше за счет более крупных исключений (?)\n",
    "\n",
    "Вопрос дефолтного связывания паттернов с правилами допускает множество решений\n",
    "и остается открытым.\n",
    "В любом случае пользователь сможет создавать исключения паттернов и дополнительные правила,\n",
    "тем самым пополняя базу данных переводчика,\n",
    "а также менять связи паттернов с правилами для своего текста, сохранять эти связи,\n",
    "и применять к другим текстам.\n",
    "\n",
    "набор правил - набор правил + номер дефолтного правила\n",
    "\t\tили просто правило\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Организация кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* parse_system.py - низкоуровневые классы, токенезация и базовые функции парсинга\n",
    "* classes.py - классы частей речи\n",
    "* ru_dictionary.py - функции отображения, русские слова и функции их добавления\n",
    "* en_dictionary.py - словарь\n",
    "\n",
    "\n",
    "* en2ru.ipynb .py - описание всего, грамматика, маленькие тесты, todo\n",
    "* tests.ipynb - тесты уже имеющихся переводов\n",
    "* utils.ipynb - прочее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:58.922857Z",
     "start_time": "2019-08-13T17:26:58.910856Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''англо-русский переводчик, основанный на правилах, с простым добавлением паттернов и правил\n",
    "\n",
    "en2ru(s) -> str - переводит строку, возвращает строку\n",
    "d_en2ru(s) -> str - дополнительно печатает отладочный вывод\n",
    "pr_l_repr(s) - печатает строку в тройных кавычках\n",
    "decline(s,pads=['ip','rp','dp','vp','tp','pp']) - возвращает список склонений переведенной фразы\n",
    "parse_pat(patt,s) -> Struct - парсит строку паттерном, возвращает нестрингифицированный объект\n",
    "d_parse_pat(patt,s) -> Struct - дополнительно печатает отладочный вывод\n",
    "scheme(s) - печатает схему разбора строки\n",
    "\n",
    "паттерны:\n",
    "p_text\n",
    "p_sentence\n",
    "p_phrase\n",
    "p_verb\n",
    "...\n",
    "p_noun\n",
    "...\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.082866Z",
     "start_time": "2019-08-13T17:26:58.928857Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import parse_system\n",
    "from parse_system import S, SAttrs, ParseInfo, tokenizer, tokenize,\\\n",
    "                        ch_title, ch_sentence, ch_anti_sentence, ch_open, ch_prefix, ch_anti_prefix,\\\n",
    "                        seq, alt, p_alt, ELSE, W, D,\\\n",
    "                        warning, debug_pp, reset_globals, global_cache, RuleVars\n",
    "import classes\n",
    "from classes import StC, StNum, StNoun, StVerb, I\n",
    "import ru_dictionary\n",
    "from ru_dictionary import ruwords, CW, add_runoun2, add_skl2, make_skl2\n",
    "import en_dictionary\n",
    "from en_dictionary import dict_adj, dict_noun, dict_pronoun_ip, dict_pronoun_dp, \\\n",
    "                        dict_numeral, dict_verb_simple, dict_verb_komu, r_adj_noun, dict_other,\\\n",
    "                        add_ennoun2, add_ennoun1, add_dict_variant\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Паттерны и правила: Составные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## FAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "паттерн - функция, которая принимает s,p(токенизированную строку и позицию в ней)   \n",
    "и возвращает список результатов, а именно список пар (позиция окончания разбора, результат)  \n",
    "токены - объекты типа S - нормализованные (т.е. в нижнем регистре) строки с дополнительными атрибутами, а именно префикс из пробельных символов, модификаторы, восстанавливающие исходный регистр слова, тэги, в которые обёрнуто это слово (тэги пока не реализованы)  \n",
    "если `f` - паттерн, то `f(s,p)` - его вызов, или его результат  \n",
    "длиной результата будем называть разность позиции окончания разбора и стартовой позиции.\n",
    "\n",
    "Терминальные паттерны:  \n",
    "`W('строка')` - считывает 1 токен, который должен совпадать со сторокой, возвращает его в непреобразованном виде, а именно объект класса S\n",
    "\n",
    "`D(dict_smth)` - считывает 1 токен, ищет его в словаре `dict_smth`, и если находит, возвращает то, что ему сопоставлено.  \n",
    "список доступных словарей можно посмотреть в en_dictionary.py\n",
    "\n",
    "Нетерминальные паттрны:  \n",
    "`alt(patt1,patt2,ELSE,patt3,patt4)` - список альтернатив, объединяет результаты паттернов. `ELSE` - ключевое слово-разделитель. Паттрены слева от ELSE являются исключениями паттернов справа, регулярных паттернов. Исключения могут отсутствовать, в этом случае `ELSE` указывать не нужно. Если ни один из регулярных паттернов не разобран, паттерны-исключения парсится не будут. Результаты паттернов исключений замещают результаты регулярных паттернов, но только той же самой длины прочтения. Если результат исключения не может заместить ни один из регулярных результатов, генерируется предупреждение.  \n",
    "`p_alt(s,p,patt1,patt2,ELSE,patt3,patt4)` эквивалентен `alt(patt1,patt2,ELSE,patt3,patt4)(s,p)`\n",
    "\n",
    "`seq([patt1,patt2,patt3],rule)`(где `def rule(rez1,rez2,rez3)`) - последовательность паттернов. Генерируются все возможные комбинации результатов, и каждая последовательность передается в правило, и из этих результатов составляется окончательный результат.  \n",
    "Если нужно применить правило к результату одного паттерна, то для этого его надо поместить в последовательность из дного этого паттерна `seq([patt1],rule)` (где `rule(rez1)`)\n",
    "\n",
    "Правила:  \n",
    "правило получает результаты последовательно разобранных паттернов, и формирует из них единый объект, возможно предварительно проведя некоторые проверки, и сообщив их результаты warning-ом  \n",
    "\n",
    "объект должен быть класса (или подкласса Struct)  \n",
    "вот типичный синтаксис:  \n",
    "```\n",
    "return StSmth([\n",
    "    I(tag = rez1),\n",
    "    I(tag = rez2),\n",
    "    I(tag = rez3),\n",
    "],struct_params...)\n",
    "```\n",
    "-- это структура, состоящая из последовательности rez1, rez2, rez3  \n",
    "доступные типы структур, а также их синтаксис (`tag`, `struct_params`) можно посмотреть в classes.py  \n",
    "`tag` задает тип взаимодействия между родительской и дочерней структурой\n",
    "\n",
    "входные результаты можно указывать в любом порядке  \n",
    "можно менять их параметры: `I(dep = rez1, param1=new_val_1, param2 = new_val_2...)`  \n",
    "можно менять их атрибуты:  \n",
    "*    добавлять атрибуты с более правого объекта `I(dep = rez1, attrs_from_right=rez2.attrs)`  \n",
    "*    добавлять атрибуты с более левого объекта `I(dep = rez2, attrs_from_left=rez1.attrs)` \n",
    "\n",
    "можно использовать фиксированные слова `I(nodep = S('строка', rez3.attrs))`, при этом снабжая (или не снабжая) их атрибутами одного из входных результатов.  \n",
    "можно использовать фиксированные объекты из словаря `I(dep = CW('кошка',rez3))`, при этом снабжая (или не снабжая) их атрибутами одного из входных результатов (в данном случае это rez3).  \n",
    "не обязательно все входящие результаты должны присутствовать в итоговом объекте. Чтобы их атрибуты не потерялись, их атрибуты можно добавлять к другим результатам, как показано выше.\n",
    "\n",
    "Может быть так, что чтруктура конструируется из нескольких фиксированных слов и одного результата. В этом случае мы часто хотим атрибуты этого результата распространить на всю структуру. Для этого в параметры структуры нужно добавить `pull_attrs=N`, где `N` - номер этого результата (нумерация с 0).\n",
    "Например\n",
    "```\n",
    "I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "]), pull_attrs=1 )\n",
    "```\n",
    "Если указать `pull_attrs=1`, то '__Kate__' будет переведено как '__у Кати__'  \n",
    "Если не указать `pull_attrs=1`, то '__Kate__' будет переведено как 'у __Кати__'\n",
    "\n",
    "Если требуется вернуть всего лишь один входной результат, изменив его параметры, он всё равно должен быть обёрнут в новую структуру\n",
    "\n",
    "Ну и, само собой, можно создавать структуры внутри которых содержатся структуры.\n",
    "\n",
    "------\n",
    "можно менять параметры входных объектов, но т.к. объекты кэшируются, их нельзя изменять, но можно указать, какие параметры будут изменены перед преобразованием объекта в строку, но для этого объект должен быть помещен в \n",
    "\n",
    "\n",
    "-Когда в правилах использовать S а когда один из классов Struct ?\n",
    "\n",
    "-S используется для неизменяемых узлов-листьев. Во всех остальных случаях используется один из классов Struct\n",
    "\n",
    "pull_attrs - если имеющуюся структуру надо расширить константами и распространить attrs структуры на расширенную структуру - смотри пример в как в r_U_noun_EST_noun\n",
    "значение pull_attrs задается равным исходной структуры в расширенной\n",
    "\n",
    "attrs_from_left/attrs_from_right - если в правиле один из аргументов пропадает, то его атрибуты желательно добавить к другому аргументу. \n",
    "Синтаксис `I(tag=remain_arg, ... , attrs_from_left=lost_arg.attrs)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.243875Z",
     "start_time": "2019-08-13T17:26:59.087866Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, S('cat',SAttrs('',set(),set())))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W('cat')(tokenize('cat'),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### dict_noun\n",
    "###### dict_pronoun_ip\n",
    "###### dict_pronoun_dp\n",
    "###### dict_adj\n",
    "###### dict_numeral\n",
    "###### dict_verb_simple\n",
    "###### dict_verb_komu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_numeral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.340881Z",
     "start_time": "2019-08-13T17:26:59.248875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_numeral(s,p):\n",
    "    return D(dict_numeral)(s,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.557893Z",
     "start_time": "2019-08-13T17:26:59.348881Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#2->\n",
    "@debug_pp\n",
    "def p_adj(s,p):\n",
    "    return D(dict_adj)(s,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun-like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_adj_noun3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.673900Z",
     "start_time": "2019-08-13T17:26:59.562893Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_adj_noun3(s,p): return p_alt(s,p,\n",
    "    seq([ alt(W('an'),W('a')), p_noun3 ],r_A_noun),\n",
    "    seq([ W('the')           , p_noun3 ],r_THE_noun),\n",
    "    seq([ W('good'), W('morning') ],r_GOOD_MORNING),             \n",
    "ELSE,\n",
    "    seq([ p_adj, p_noun3 ],r_adj_noun)\n",
    ")\n",
    "# r_adj_noun определен в en_dictionary.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.808908Z",
     "start_time": "2019-08-13T17:26:59.677900Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# исключения\n",
    "def r_A_noun(_a,_n): return StNoun([\n",
    "    I(maindep=_n,         attrs_from_left=_a.attrs)\n",
    "])\n",
    "def r_THE_noun(_a,_n): return StNoun([\n",
    "    I(maindep=_n,         attrs_from_left=_a.attrs)\n",
    "])\n",
    "\n",
    "def r_GOOD_MORNING(_g,_m):  return r_adj_noun(\n",
    "    CW('добрый',_g),\n",
    "    CW('утро',_m)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:26:59.932915Z",
     "start_time": "2019-08-13T17:26:59.814908Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun3(s,p): return p_alt(s,p,\n",
    "    p_adj_noun3, #ELSE, # переход к следующему уровню\n",
    "    p_adj, #ELSE, # переход к следующему уровню\n",
    "    p_numeral,\n",
    "    D(dict_noun)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.057922Z",
     "start_time": "2019-08-13T17:26:59.936915Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun2_1(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun3, p_dops ], r_noun_dops), #ELSE, # переход к следующему уровню\n",
    "    p_noun3\n",
    ")\n",
    "def r_noun_dops(n,d): return StNoun([\n",
    "    I(maindep=n),\n",
    "    I(nodep=d)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun2\n",
    "###### p_dop_noun2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.155927Z",
     "start_time": "2019-08-13T17:27:00.061922Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun2(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun2_1, p_numeral ], r_noun_numeral), #ELSE, # переход к следующему уровню\n",
    "    p_noun2_1\n",
    ")\n",
    "@debug_pp\n",
    "def p_dop_noun2(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun3, p_numeral ], r_noun_numeral), #ELSE, # переход к следующему уровню\n",
    "    p_noun3\n",
    ")\n",
    "def r_noun_numeral(n,num): return StNoun([\n",
    "    I(maindep=n),\n",
    "    I(nomer=num)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun1\n",
    "###### p_dop_noun1\n",
    "###### p_noun1_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.289935Z",
     "start_time": "2019-08-13T17:27:00.159928Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun1(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_noun2,\n",
    "    D(dict_pronoun_dp)\n",
    ")\n",
    "@debug_pp\n",
    "def p_dop_noun1(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_dop_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_dop_noun2,\n",
    "    D(dict_pronoun_dp)\n",
    ")\n",
    "@debug_pp\n",
    "def p_noun1_ip(s,p): return p_alt(s,p,\n",
    "    seq([ p_numeral, p_noun2 ], r_numeral_noun), #ELSE, # переход к следующему уровню\n",
    "    p_noun2,\n",
    "    D(dict_pronoun_ip)\n",
    ")\n",
    "def r_numeral_noun(num,n):\n",
    "    if num.chis!=n.chis :\n",
    "        warning('не совпадают числа числ. и сущ.:'+str(num)+str(n))\n",
    "    return StNum([\n",
    "        I(quantity=num,            chis=n.chis, rod=n.rod, odush=n.odush ),\n",
    "        I(maindep=n)\n",
    "    ],quantity=num.quantity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun\n",
    "###### p_dop_noun\n",
    "###### p_noun_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.406942Z",
     "start_time": "2019-08-13T17:27:00.293935Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun1, W('and'), p_noun ],r_noun_and_noun  ),\n",
    "        seq([ p_noun1, W(',')  , p_noun ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_noun1\n",
    "    )\n",
    "@debug_pp\n",
    "def p_dop_noun(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_dop_noun1, W('and'), p_dop_noun ],r_noun_and_noun  ),\n",
    "        seq([ p_dop_noun1, W(',')  , p_dop_noun ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_dop_noun1\n",
    "    )\n",
    "@debug_pp\n",
    "def p_noun_ip(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun1_ip, W('and'), p_noun_ip ],r_noun_and_noun  ),\n",
    "        seq([ p_noun1_ip, W(',')  , p_noun_ip ],r_noun_comma_noun),\n",
    "                 #ELSE, # переход к следующему уровню\n",
    "                 # идет конфликт с and-ом из глаголов\n",
    "        p_noun1_ip\n",
    "    )\n",
    "def r_noun_and_noun(sn,a,n):    return StNoun([\n",
    "    I(dep=sn),\n",
    "    I(nodep=S('и',a.attrs)),\n",
    "    I(dep=n)\n",
    "],c='mn', p='ip',o=False,r='m')\n",
    "def r_noun_comma_noun(sn,c,n):    return StNoun([\n",
    "    I(dep=sn),\n",
    "    I(punct=S(',',c.attrs)),\n",
    "    I(dep=n)\n",
    "],c='mn', p='ip',o=False,r='m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Существительные в разных формах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.522948Z",
     "start_time": "2019-08-13T17:27:00.410942Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_noun_dp(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_pronoun_dp)],r_noun_dp), \n",
    "    seq([ W('to'), p_noun ],r_TO_noun_dp)\n",
    ")\n",
    "def r_noun_dp(_n): return StNoun([\n",
    "    I(maindep=_n,         pad='dp')\n",
    "])\n",
    "\n",
    "def r_TO_noun_dp(_t,_n): return StNoun([\n",
    "    I(maindep=_n,         pad='dp', attrs_from_left=_t.attrs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_IN_THE_STREET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.639955Z",
     "start_time": "2019-08-13T17:27:00.526949Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# на (определенной) улице\n",
    "@debug_pp\n",
    "def pe_IN_THE_STREET(s,p): return p_alt(s,p,\n",
    "    seq([W('in'), W('the'), W('street') ],r_NA_X_ULITSE),\n",
    ")\n",
    "def r_NA_X_ULITSE(v,_a,_U): return StC([\n",
    "    I(nodep=S('на',v.attrs)),\n",
    "    I(nodep=CW('улица',_U),pad='pp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_IN_adj_STREET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.741961Z",
     "start_time": "2019-08-13T17:27:00.643955Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# на какой-то улице\n",
    "@debug_pp\n",
    "def pe_IN_adj_STREET(s,p): return p_alt(s,p,\n",
    "    pe_IN_THE_STREET,\n",
    "    ELSE,\n",
    "    seq([W('in'), p_adj, W('street') ],r_NA_adj_ULITSE),\n",
    ")\n",
    "def r_NA_adj_ULITSE(v,_a,_U): return StC([\n",
    "    I(nodep=S('на',v.attrs)),\n",
    "    I(nodep = StNoun([\n",
    "        I(dep=_a,\n",
    "            rod='g',\n",
    "            chis='ed',\n",
    "            pad='ip'),\n",
    "        I(maindep=CW('улица',_U))\n",
    "    ]),pad='pp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.850967Z",
     "start_time": "2019-08-13T17:27:00.745961Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# где\n",
    "@debug_pp\n",
    "def p_where(s,p): return p_alt(s,p,\n",
    "    pe_IN_adj_STREET,\n",
    "    ELSE,\n",
    "    seq([W('in'), p_dop_noun ],r_V_noun_pp),\n",
    "    seq([W('on'), p_dop_noun ],r_NA_noun_pp),\n",
    ")\n",
    "def r_V_noun_pp(v,_n): return StC([\n",
    "    I(nodep=S('в',v.attrs)),\n",
    "    I(nodep=_n,  pad='pp'),\n",
    "])\n",
    "def r_NA_noun_pp(v,_n): return StC([\n",
    "    I(nodep=S('на',v.attrs)),\n",
    "    I(nodep=_n,  pad='pp'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_dop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:00.958973Z",
     "start_time": "2019-08-13T17:27:00.854967Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# одно дополнение\n",
    "@debug_pp\n",
    "def p_dop(s,p): return p_alt(s,p,\n",
    "    seq([W('to'), p_dop_noun ],r_X_noun_dp),\n",
    "    seq([W('from'),p_dop_noun],r_IZ_noun),\n",
    "    seq([W('with'),p_dop_noun],r_S_noun_tp),\n",
    "    p_where\n",
    ")\n",
    "def r_X_noun_dp(x,n): return StNoun([\n",
    "    I(maindep=n,  pad='dp',         attrs_from_left=x.attrs)\n",
    "])\n",
    "def r_IZ_noun(iz,n): return StNoun([\n",
    "    I(nodep=S('из',iz.attrs)),\n",
    "    I(maindep=n,  pad='rp')\n",
    "])\n",
    "def r_S_noun_tp(s,n): return StNoun([\n",
    "    I(nodep=S('с',s.attrs)),\n",
    "    I(maindep=n,  pad='tp')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_dops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:01.058979Z",
     "start_time": "2019-08-13T17:27:00.962974Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# последовательность дополнений\n",
    "@debug_pp\n",
    "def p_dops(s,p): return p_alt(s,p,\n",
    "    p_dop,\n",
    "    seq([p_dop, p_dops], r_seq_dops)\n",
    ")\n",
    "def r_seq_dops(d1,d2): return StC([\n",
    "    I(nodep=d1),\n",
    "    I(nodep=d2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## have/has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:01.199987Z",
     "start_time": "2019-08-13T17:27:01.068980Z"
    }
   },
   "outputs": [],
   "source": [
    "px_HAVE_HAS = alt( W('have'), W('has') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_have_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:01.541007Z",
     "start_time": "2019-08-13T17:27:01.204987Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_have_question(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([px_HAVE_HAS,p_noun_ip,p_noun],r_have_question),\n",
    "        seq([W('how'), W('many'), p_noun, px_HAVE_HAS, p_noun_ip], rv_HOW_MANY_noun_HAVE_noun)\n",
    "    )\n",
    "\n",
    "def r_have_question(_h,_n1,_n2):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h.attrs)),\n",
    "    I(nodep=_n2)\n",
    "])\n",
    "def r_SKOLKO_noun_U_noun(how,many,_n1,have,_n2):  return StC([\n",
    "    I(nodep=S('сколько',how.attrs), attrs_from_right=many.attrs),\n",
    "    I(nodep=_n1, pad='rp'),\n",
    "    I(nodep=S('у')),\n",
    "    I(nodep=_n2,   pad='rp', npad='n' )# у Него\n",
    "])\n",
    "def r_SKOLKO_U_noun_noun(how,many,_n1,have,_n2):  return StC([\n",
    "    I(nodep=S('сколько',how.attrs), attrs_from_right=many.attrs),\n",
    "    I(nodep=S('у')),\n",
    "    I(nodep=_n2,   pad='rp', npad='n' ),# у Него\n",
    "    I(nodep=_n1, pad='rp'),\n",
    "])\n",
    "rv_HOW_MANY_noun_HAVE_noun = [2,r_SKOLKO_noun_U_noun,r_SKOLKO_U_noun_noun]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_noun_HAVE_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:01.852024Z",
     "start_time": "2019-08-13T17:27:01.545007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_HAVE_noun(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun_ip, px_HAVE_HAS,          p_noun ],rv_noun_HAVE_noun),\n",
    "        seq([ p_noun_ip, px_HAVE_HAS, W('no'), p_noun ],r_U_noun_NET_noun)\n",
    "#        seq([ p_noun, p_HAVE_HAS,          p_pronoun_dp ],r_noun_EST_U_noun),\n",
    "#        seq([ p_noun, p_HAVE_HAS, W('no'), p_pronoun_dp ],r_noun_NET_U_noun)\n",
    "    )\n",
    "def r_U_noun_EST_noun(_n1_,_h_,_n2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h_.attrs)),\n",
    "    I(nodep=_n2_)\n",
    "])\n",
    "def r_U_noun_noun(_n1_,_h_,_n2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1, attrs_from_right = _h_.attrs ),\n",
    "    I(nodep=_n2_)\n",
    "])\n",
    "rv_noun_HAVE_noun = RuleVars([2,r_U_noun_noun, r_U_noun_EST_noun])\n",
    "\n",
    "def r_U_noun_NET_noun(_n1_,_h_,_no_,_n2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('нет',_h_.attrs)),\n",
    "    I(nodep=_n2_,        pad='rp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_noun_HAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.107039Z",
     "start_time": "2019-08-13T17:27:01.856025Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_HAVE(s,p):\n",
    "    return p_alt(s,p,\n",
    "        seq([ p_noun_ip, px_HAVE_HAS          ],r_U_noun_EST),\n",
    "        seq([ p_noun_ip, px_HAVE_HAS, alt(W('no'),W('not')) ],r_U_noun_NET)\n",
    "#        seq([ p_noun, p_HAVE_HAS,          p_pronoun_dp ],r_noun_EST_U_noun),\n",
    "#        seq([ p_noun, p_HAVE_HAS, W('no'), p_pronoun_dp ],r_noun_NET_U_noun)\n",
    "    )\n",
    "def r_U_noun_EST(_n1_,_h_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('есть',_h_.attrs))\n",
    "])\n",
    "\n",
    "def r_U_noun_NET(_n1_,_h_,_no_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=S('у')),\n",
    "        I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "    ]), pull_attrs=1 ),\n",
    "    I(nodep=S('нет',_h_.attrs))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_HAVE_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.210045Z",
     "start_time": "2019-08-13T17:27:02.111039Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_HAVE_noun(s,p): \n",
    "    return p_alt(s,p,\n",
    "        seq([px_HAVE_HAS,          p_noun], r_IMET_noun_vp),\n",
    "        seq([px_HAVE_HAS, W('no'), p_noun], r_NE_IMET_noun_vp),\n",
    "        seq([px_HAVE_HAS],r_IMET),\n",
    "        seq([px_HAVE_HAS,alt(W('no'),W('not'))],r_NE_IMET)\n",
    "    )\n",
    "def r_NE_IMET_noun_vp(_v,no,_n): return StVerb([\n",
    "    I(nodep=S('не',no.attrs)),\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "    I(vp=_n,   pad='vp')\n",
    "])\n",
    "def r_IMET_noun_vp(_v,_n): return StVerb([\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "    I(vp=_n,   pad='vp')\n",
    "])\n",
    "def r_IMET(_v): return StVerb([\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "])\n",
    "def r_NE_IMET(_v,no): return StVerb([\n",
    "    I(nodep=S('не',no.attrs)),\n",
    "    I(maindep=CW('иметь',_v)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_noun_TOBE_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.318051Z",
     "start_time": "2019-08-13T17:27:02.214045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_TOBE_where(s,p): return p_alt(s,p,\n",
    "    seq([W('this'),p_TOBE,p_where],re_ETO_X_where),\n",
    "    seq([W('that'),p_TOBE,p_where],re_TO_X_where),\n",
    "    ELSE,\n",
    "    seq([p_noun_ip,p_TOBE,p_where],r_noun_X_where)\n",
    ")\n",
    "\n",
    "def r_noun_X_where(_n,x,_w): return StC([\n",
    "    I(nodep=_n),\n",
    "    I(nodep=_w, attrs_from_left=x.attrs)\n",
    "])\n",
    "def re_ETO_X_where(_n,x,_w): return StC([\n",
    "    I(nodep=CW('этот',_n), rod='s'),\n",
    "    I(nodep=_w, attrs_from_left=x.attrs)\n",
    "])\n",
    "def re_TO_X_where(_n,x,_w): return StC([\n",
    "    I(nodep=CW('тот',_n, rod='s')),\n",
    "    I(nodep=_w, attrs_from_left=x.attrs)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### pe_noun_TOBE_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.434058Z",
     "start_time": "2019-08-13T17:27:02.322051Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def pe_noun_TOBE_noun(s,p): return p_alt(s,p,\n",
    "    seq([W('this'),p_TOBE,p_noun],re_ETO_X_noun),\n",
    "    seq([W('that'),p_TOBE,p_noun],re_TO_X_noun),\n",
    "    ELSE,\n",
    "    seq([p_noun_ip,p_TOBE,p_noun],r_noun_X_noun)\n",
    ")\n",
    "\n",
    "def r_noun_X_noun(_n1,_tobe,_n2): return StC([\n",
    "    I(nodep=_n1),\n",
    "    I(nodep=S('--',_tobe.attrs)),\n",
    "    I(nodep=_n2, rod=_n1.rod)\n",
    "])\n",
    "def re_ETO_X_noun(_n1,_tobe,_n2): return StC([\n",
    "    I(nodep=CW('этот',_n1), rod='s'),\n",
    "    I(nodep=_n2, attrs_from_left = _tobe.attrs)\n",
    "])\n",
    "def re_TO_X_noun(_n1,_tobe,_n2): return StC([\n",
    "    I(nodep=CW('тот',_n1), rod='s'),\n",
    "    I(nodep=_n2, attrs_from_left = _tobe.attrs)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_TOBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.567065Z",
     "start_time": "2019-08-13T17:27:02.438058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_TOBE(s,p): return p_alt(s,p,\n",
    "    seq([W('be')],r_EST),\n",
    "    seq([W('am')],r_EST),\n",
    "    seq([W('is')],r_EST),\n",
    "    seq([W('are')],r_EST)\n",
    ")\n",
    "def r_EST(_v): return StVerb([\n",
    "    I(maindep=CW('есть (быть)',_v)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_TOBE_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.676072Z",
     "start_time": "2019-08-13T17:27:02.571065Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_TOBE_noun(s,p): return p_alt(s,p,\n",
    "    seq([p_TOBE, p_noun], rv_TOBE_noun),\n",
    "#    seq([p_tobe, W('no'), alt(p_noun,D(dict_pronoun_dp))], r_NE_IMET_noun_vp),\n",
    "    p_TOBE\n",
    ")\n",
    "def r_EST_noun_ip(_v,_n): return StVerb([\n",
    "    I(maindep=CW('есть (быть)',_v)),\n",
    "    I(ip=_n,   pad='ip')\n",
    "])\n",
    "def r_JAVLYATSA_noun_tp(_v,_n): return StVerb([\n",
    "    I(maindep=CW('являться',_v)),\n",
    "    I(tp=_n,   pad='tp')\n",
    "])\n",
    "rv_TOBE_noun = [1, r_EST_noun_ip, r_JAVLYATSA_noun_tp]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глагол с дополнениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.831080Z",
     "start_time": "2019-08-13T17:27:02.680072Z"
    }
   },
   "outputs": [],
   "source": [
    "# разделяемые правила\n",
    "def r_verb_noun_vp(_v,_p): return StVerb([\n",
    "    I(maindep= _v),\n",
    "    I(vp=_p,   pad='vp')\n",
    "])\n",
    "def r_verb_noun_dp(_v,_p): return StVerb([\n",
    "    I(maindep= _v),\n",
    "    I(dp=_p,   pad='dp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb3_komu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:02.934086Z",
     "start_time": "2019-08-13T17:27:02.835081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать кому-то\n",
    "@debug_pp\n",
    "def p_verb3_komu(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_verb_komu), p_noun_dp], r_verb_noun_dp),\n",
    "    D(dict_verb_komu)\n",
    ")\n",
    "# r_verb_noun_dp - разделяемое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb3_komu_chto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.017091Z",
     "start_time": "2019-08-13T17:27:02.938086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать кому-то что-то\n",
    "@debug_pp\n",
    "def p_verb3_komu_chto(s,p): return p_alt(s,p,\n",
    "    seq([p_verb3_komu,                D(dict_pronoun_dp)], r_verb_noun_dp),\n",
    "    ELSE,\n",
    "    seq([ p_verb3_komu,                 p_noun        ], r_verb_noun_vp),\n",
    "    seq([ p_verb3_komu, W(':'),         p_phrase      ], r_verb_c_phrase),\n",
    "    seq([ p_verb3_komu,         W('\"'), p_text, W('\"')], r_verb_q_text),\n",
    "    seq([ p_verb3_komu, W(':'), W('\"'), p_text, W('\"')], r_verb_c_q_text), \n",
    "    p_verb3_komu\n",
    ")\n",
    "# r_verb_noun_vp - разделяемое\n",
    "# r_verb_noun_dp - разделяемое\n",
    "def r_verb_c_phrase(_v,c,_p): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=c),\n",
    "    I(nodep=_p)\n",
    "])\n",
    "def r_verb_q_text(_v,q1,_p,q2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=q1, add_changers={ch_open}),\n",
    "    I(nodep=_p),\n",
    "    I(punct=q2),\n",
    "])\n",
    "def r_verb_c_q_text(_v,c,q1,_p,q2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(punct=c),\n",
    "    I(punct=q1, add_changers={ch_open}),\n",
    "    I(nodep=_p),\n",
    "    I(punct=q2),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.118097Z",
     "start_time": "2019-08-13T17:27:03.021091Z"
    },
    "code_folding": [
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# глагол с дополнением\n",
    "@debug_pp\n",
    "def p_verb2(s,p): return p_alt(s,p,\n",
    "    seq([D(dict_verb_simple),W('from'),p_noun,W('to'), p_noun],re_verb_OT_noun_DO_noun),\n",
    "    ELSE,\n",
    "    seq([D(dict_verb_simple), p_dops],r_verb_dops),\n",
    "    D(dict_verb_simple)\n",
    ")\n",
    "\n",
    "def r_verb_dops(_v,_d): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(nodep=_d)\n",
    "])\n",
    "def re_verb_OT_noun_DO_noun(_v,ot,_n1,do,_n2): return StVerb([\n",
    "    I(maindep=_v),\n",
    "    I(nodep=S('от',ot.attrs)),\n",
    "    I(nodep=_n1,  pad='rp'),\n",
    "    I(nodep=S('до',do.attrs)),\n",
    "    I(nodep=_n2,  pad='rp')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb3_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.216102Z",
     "start_time": "2019-08-13T17:27:03.122097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать что-то\n",
    "@debug_pp\n",
    "def p_verb3_simple(s,p): return p_alt(s,p,\n",
    "    seq([p_verb2, p_noun], r_verb_noun_vp),\n",
    "    p_verb2\n",
    ")\n",
    "# r_verb_noun_vp - разделяемое"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.309108Z",
     "start_time": "2019-08-13T17:27:03.220103Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать (кому-то что-то)\n",
    "@debug_pp\n",
    "def p_verb3_1(s,p): return p_alt(s,p,\n",
    "    p_verb3_simple,\n",
    "    p_verb3_komu_chto,\n",
    "    p_HAVE_noun,\n",
    "    p_TOBE_noun,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.402113Z",
     "start_time": "2019-08-13T17:27:03.313108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# могу сделать\n",
    "@debug_pp\n",
    "def p_verb3(s,p): return p_alt(s,p,\n",
    "    seq([W('can'), p_verb3_1],r_CAN_verb),\n",
    "    p_verb3_1\n",
    ")\n",
    "def r_CAN_verb(c,v): return StVerb([\n",
    "    I(maindep=CW('мочь',c)),\n",
    "    I(nodep=v,   form='neopr')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Глагол(ы) с подлежащим, или другой формы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_noun_verb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.518120Z",
     "start_time": "2019-08-13T17:27:03.406113Z"
    },
    "code_folding": [
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# некто делает\n",
    "@debug_pp\n",
    "def p_noun_verb1(s,p): return p_alt(s,p,\n",
    "    pe_noun_HAVE_noun,\n",
    "    pe_noun_HAVE,\n",
    "    pe_noun_TOBE_noun,\n",
    "ELSE,\n",
    "    pe_noun_TOBE_where,\n",
    "    seq([ p_noun_ip , p_verb3 ],r_noun_verb),\n",
    "    seq([ p_noun_ip, p_verb3, W('too') ],r_noun_TOZHE_verb), #ELSE, # переход к следующему уровню\n",
    ")\n",
    "\n",
    "def r_noun_verb(n,v): return StVerb([\n",
    "    I(ip=n),\n",
    "    I(main=v,   form='nast', pers=n.pers, chis=n.chis, rod=n.rod)\n",
    "])\n",
    "def r_noun_TOZHE_verb(_n, _v, _t): return StVerb([\n",
    "    I(ip=_n),\n",
    "    I(nodep=S('тоже',_t.attrs)),\n",
    "    I(main=_v,   form='nast', pers=_n.pers, chis=_n.chis, rod=_n.rod)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.602124Z",
     "start_time": "2019-08-13T17:27:03.522120Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# некто делает/ делать/ делай\n",
    "@debug_pp\n",
    "def p_verb1(s,p): return p_alt(s,p,\n",
    "    p_noun_verb1,\n",
    "    seq([ W('to'), p_verb3 ],r_to_verb),   #ELSE, # переход к следующему уровню\n",
    "    seq([ p_verb3 ],rv_rule_povel_verb)\n",
    ")\n",
    "def r_to_verb(_t,_v): return StVerb([\n",
    "    I(maindep=_v,         form='neopr', attrs_from_left=_t.attrs)\n",
    "])\n",
    "\n",
    "def r_povel_verb_ed(_v): return StVerb([\n",
    "    I(maindep=_v, asp='sov', form='povel',chis='ed') #\n",
    "])\n",
    "def r_povel_verb_mn(_v): return StVerb([\n",
    "    I(maindep=_v, asp='sov', form='povel',chis='mn') # asp='sov',\n",
    "])\n",
    "rv_rule_povel_verb = [1,r_povel_verb_ed,r_povel_verb_mn]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.694130Z",
     "start_time": "2019-08-13T17:27:03.606125Z"
    },
    "code_folding": [
     10,
     17
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# сделать одно и/но сделать сдругое\n",
    "@debug_pp\n",
    "def p_verb(s,p): return p_alt(s,p,\n",
    "    seq([ p_noun_ip, px_HAVE_HAS, p_noun1, W(',')  , p_noun,  W('and'), p_verb1 ],re_U_noun_EST_noun_C_noun_I_verb),\n",
    "    ELSE,\n",
    "    seq([ p_verb1, W(','), p_verb1 ]          ,r_verb_c_verb),   \n",
    "    seq([ p_verb1, W(','), W('but'), p_verb1 ],r_verb_NO_verb),   \n",
    "    seq([ p_verb1, W('and'), p_verb1 ]        ,r_verb_I_verb),\n",
    "    #ELSE, # переход к следующему уровню\n",
    "    p_verb1\n",
    ")\n",
    "\n",
    "def r_verb_NO_verb(_v1_,_c_,_but_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=_c_),\n",
    "    I(nodep=S('но',_c_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def r_verb_c_verb(_v1_,_c_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=_c_),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def r_verb_I_verb(_v1_,_i_,_v2_):    return StC([\n",
    "    I(nodep=_v1_),\n",
    "    I(nodep=S('и',_i_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])\n",
    "\n",
    "def re_U_noun_EST_noun_C_noun_I_verb(_n1_,_h_,sn,c,n,_i_,_v2_):    return StC([\n",
    "    I(nodep=StC([\n",
    "        I(nodep=StC([\n",
    "            I(nodep=S('у')),\n",
    "            I(nodep=_n1_,   pad='rp', npad='n' )# у Него\n",
    "        ]), pull_attrs=1 ),\n",
    "        I(nodep=S('есть',_h_.attrs)),\n",
    "        I(nodep=StNoun([\n",
    "            I(dep=sn),\n",
    "            I(punct=S(',',c.attrs)),\n",
    "            I(dep=n)\n",
    "        ],c='mn', p='ip',o=False,r='m'))\n",
    "    ])),\n",
    "    I(nodep=S('и',_i_.attrs)),\n",
    "    I(nodep=_v2_)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фразы, предложения, текст"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.785135Z",
     "start_time": "2019-08-13T17:27:03.698130Z"
    },
    "code_folding": [
     14,
     20,
     26
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_phrase(s,p): \n",
    "    return p_alt(s,p,\n",
    "        p_verb,    #ELSE,\n",
    "        p_noun_ip,    #ELSE,\n",
    "        p_noun_dp, #ELSE,\n",
    "        p_dop,\n",
    "        D(dict_other),\n",
    "        seq([W('yes'),W(','),p_verb],r_DA_COMMA_verb),\n",
    "        seq([W('no') ,W(','),p_verb],r_NET_COMMA_verb),\n",
    "        seq([p_noun  ,W(','),p_verb],r_noun_COMMA_verb),\n",
    "    )\n",
    "\n",
    "def r_DA_COMMA_verb(yes,comma,_v):    return StC([\n",
    "    I(nodep=S('да',yes.attrs)),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])\n",
    "\n",
    "def r_NET_COMMA_verb(no,comma,_v):    return StC([\n",
    "    I(nodep=S('нет',no.attrs)),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])\n",
    "\n",
    "def r_noun_COMMA_verb(_n,comma,_v):    return StC([\n",
    "    # обращение\n",
    "    I(nodep=_n),\n",
    "    I(nodep=comma),\n",
    "    I(nodep=_v)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_question_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.876140Z",
     "start_time": "2019-08-13T17:27:03.789135Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_question_phrase(s,p): \n",
    "    return p_have_question(s,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:03.959145Z",
     "start_time": "2019-08-13T17:27:03.880140Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_proper={}# имена собственные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.069151Z",
     "start_time": "2019-08-13T17:27:03.963145Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_sentence(s,p):\n",
    "    first_capital = s[p]=='I' or ch_title in s[p].attrs.changers\n",
    "    def r_sentence(ph,d):\n",
    "        rez=StC([I(nodep=ph),I(punct=d)])\n",
    "        if first_capital: rez.attrs.changers|={ch_sentence}\n",
    "        rez.attrs.pre = prefix + rez.attrs.pre\n",
    "        rez.attrs.changers|={ch_prefix}\n",
    "        return rez\n",
    "    restore_title=False\n",
    "    if ch_title in s[p].attrs.changers and s[p] not in dict_proper:\n",
    "        s[p].attrs.changers-={ch_title}\n",
    "        s[p].attrs.changers|={ch_anti_sentence}# хак, реализованный в SAttrs.join()\n",
    "        restore_title=True\n",
    "    prefix = s[p].attrs.pre\n",
    "    s[p].attrs.changers |= {ch_anti_prefix}\n",
    "    try:\n",
    "        rezs=p_alt(s,p,\n",
    "            seq([p_phrase,alt(W('.'),W('!'),W(';'))],r_sentence),\n",
    "            seq([p_question_phrase,W('?')],r_sentence)\n",
    "        )\n",
    "    finally:# не работает, т.к. всё закешировалось\n",
    "        if restore_title:\n",
    "            s[p].attrs.changers|={ch_title}\n",
    "            s[p].attrs.changers-={ch_anti_sentence}\n",
    "        s[p].attrs.pre = prefix\n",
    "    return rezs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.152156Z",
     "start_time": "2019-08-13T17:27:04.073151Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "@debug_pp\n",
    "def p_text(s,p):\n",
    "    '''или последовательность предложений или 1 фраза\n",
    "    p_text::= p_sentence* | p_phrase\n",
    "    '''\n",
    "    rez=[]\n",
    "    while p<len(s):\n",
    "        rezs=maxlen_filter(p_sentence,s,p)\n",
    "        if len(rezs)==0: break\n",
    "        p1,r1=rezs[0] # отбрасываем остальные результаты\n",
    "        p=p1\n",
    "        rez.append(I(nodep=r1))\n",
    "    if len(rez)>0:\n",
    "        return [(p,StC(rez))]\n",
    "    else:\n",
    "        return maxlen_filter(alt(p_phrase,p_question_phrase),s,p)\n",
    "\n",
    "def maxlen_filter(patt,s,p):\n",
    "    '''находит самые длинные результаты, а остальные отбрасывает\n",
    "    \n",
    "    если самых длинных несколько - warning\n",
    "    '''\n",
    "    rezs=patt(s,p)\n",
    "    m=0 # самая длинная длина\n",
    "    im=set() # множество номеров результатаов с самой длинной длиной\n",
    "    for i in range(len(rezs)):\n",
    "        if rezs[i][0]>m:\n",
    "            m=rezs[i][0]\n",
    "            im={i}\n",
    "        elif rezs[i][0]==m:\n",
    "            im.add(i)\n",
    "    long_rezs= [rezs[i] for i in im]\n",
    "    if len(long_rezs)>1:\n",
    "        #print(p,m,s[p:m],SAttrs().join(s))\n",
    "        warning('multiple results:\\n'+\n",
    "            SAttrs().join(s[p:m])+'\\n'+\n",
    "            '\\n'.join(r.tostr() for void,r in long_rezs)\n",
    "        )\n",
    "            \n",
    "    return [] if len(long_rezs)==0 else long_rezs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Запуск и отладка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.244161Z",
     "start_time": "2019-08-13T17:27:04.156156Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _en2ru(s): # main\n",
    "    ''' (text|.)* + warning-и\n",
    "    '''\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    if len(s)==0:\n",
    "        warning('no tokens')\n",
    "        return ''\n",
    "    \n",
    "    ret_s = ''\n",
    "    p=0\n",
    "    while p<len(s):\n",
    "        #print('ITERATION',p)\n",
    "        rezs= p_text(s,p)\n",
    "        if len(rezs)==0:\n",
    "            warning(\"CAN'T TRANSLATE: \"+s[p])\n",
    "            ret_s += (' | ' if p>0 else '')+ s[p]\n",
    "            p+=1\n",
    "        else:\n",
    "            assert len(rezs)==1\n",
    "            p1,r1 = rezs[0]\n",
    "            #print(p,p1,r1)\n",
    "            s1 = r1.tostr()\n",
    "            #print(p,p1,r1)\n",
    "            ret_s += (' | ' if p>0 else '')+ s1\n",
    "            if p>0:\n",
    "                warning('TRANSLATION BREAKS')\n",
    "            assert p1>p, rezs\n",
    "            p=p1\n",
    "    return ret_s\n",
    "\n",
    "def en2ru(s):\n",
    "    '''переводит строку, возвращает строку'''\n",
    "    parse_system.DEBUGGING=False\n",
    "    return _en2ru(s)\n",
    "\n",
    "def d_en2ru(s):\n",
    "    '''переводит строку, возвращает строку\n",
    "    \n",
    "    дополнительно пишет отладочный вывод'''\n",
    "    l_d = parse_system.DEBUGGING\n",
    "    parse_system.DEBUGGING=True\n",
    "    try:\n",
    "        r=_en2ru(s)\n",
    "    finally:\n",
    "        parse_system.DEBUGGING=l_d\n",
    "    return r\n",
    "\n",
    "def pr_l_repr(s):\n",
    "    \"\"\"печатает строку в тройных кавычках\"\"\"\n",
    "    print(\"'''\"+s+\"'''\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.451173Z",
     "start_time": "2019-08-13T17:27:04.248161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def en2ru_with_variants(variants,s):\n",
    "    '''variants - список пар (RuleVars,n)'''\n",
    "    saves = {}\n",
    "    for k,v in variants:\n",
    "        saves[id(k)]=k.default()\n",
    "        if type(v)==str: v = ruwords[v]\n",
    "        k.select(v)\n",
    "    s = en2ru(s)\n",
    "    for k,v in variants:\n",
    "        k.select(saves[id(k)])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.560179Z",
     "start_time": "2019-08-13T17:27:04.455173Z"
    },
    "code_folding": [
     9
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decline(s,pads=['ip','rp','dp','vp','tp','pp']):\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    # добавить дочитывание точки и остаточных пробелов\n",
    "    rezs=[res for pos,res in p_noun(s,0) if pos==len(s)]\n",
    "    if len(rezs)!=1:\n",
    "        raise TextError(rezs)\n",
    "    tmp=rezs[0]\n",
    "    \n",
    "    m=[]\n",
    "    for p in pads:\n",
    "        #print(str(tmp))\n",
    "        prompt= \\\n",
    "            '' if p=='ip' else\\\n",
    "            'нет ' if p=='rp' else\\\n",
    "            'дать ' if p=='dp' else\\\n",
    "            'вижу ' if p=='vp' else\\\n",
    "            'творю ' if p=='tp' else\\\n",
    "            'думаю о ' if p=='pp' else\\\n",
    "            throw(ValueError('bad pad: '+p))\n",
    "        #rez=deepcopy(tmp)\n",
    "        tmp.pad=p\n",
    "        m.append(prompt+tmp.tostr())#        print(prompt+str(tmp))\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.693187Z",
     "start_time": "2019-08-13T17:27:04.564179Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _parse_pat(patt,s):\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    return patt(s,0)\n",
    "\n",
    "def parse_pat(patt,s):\n",
    "    '''парсит строку паттерном, и возвращает не стрингифицированный объект'''\n",
    "    parse_system.DEBUGGING=False\n",
    "    return _parse_pat(patt,s)\n",
    "\n",
    "def d_parse_pat(patt,s):\n",
    "    '''парсит строку паттерном, и возвращает не стрингифицированный объект\n",
    "    \n",
    "    дополнительно пишет отладочный вывод'''\n",
    "    l_d = parse_system.DEBUGGING\n",
    "    parse_system.DEBUGGING=True\n",
    "    try:\n",
    "        r=parsepat(s,patt)\n",
    "    finally:\n",
    "        parse_system.DEBUGGING=l_d\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:30:05.108414Z",
     "start_time": "2019-08-13T18:30:04.972406Z"
    },
    "code_folding": [
     2,
     45,
     106,
     118,
     147,
     222
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def sch_print_rez0(rez,depth,s,detailed):\n",
    "    '''выводит исходное дерево слева направо'''\n",
    "    info = rez.parse_info\n",
    "    if hasattr(info,'p_start'):\n",
    "        print('  '*depth+' '+SAttrs().join( s[info.p_start : info.p_end] ))\n",
    "    if hasattr(info,'patterns') or hasattr(info,'rule_group'):\n",
    "        if hasattr(info,'patterns'):\n",
    "            #'<'+str(id(info.patterns))+'>'+\n",
    "            patterns = ' '.join(info.patterns2str()) if detailed>=1 else info.patterns2str()[0]\n",
    "        else:\n",
    "            patterns = ''\n",
    "        if hasattr(info,'rule_group'):\n",
    "            if type(info.rule_group)==list:\n",
    "                n = info.rule_group[0] if info.rule_group[0]!=0 else 1\n",
    "                rule = info.rule_group[n]\n",
    "                rules = str(n)+'/'+str(len(info.rule_group)-1)+' '\n",
    "            else:\n",
    "                rule = info.rule_group\n",
    "                rules = ''\n",
    "            rules += (rule.__name__ if callable(rule) else str(rule))\n",
    "        else:\n",
    "            rules = ''\n",
    "        print('  '*depth +' '+ patterns+' -> '+rules)\n",
    "    print('  '*depth+'*'+str(rez))\n",
    "    print('  '*depth+'*'+repr(rez))\n",
    "\n",
    "    if hasattr(rez,'talk'):\n",
    "        for x in rez.talk:\n",
    "            sch_print_rez0(x[1],depth+1,s,detailed)\n",
    "\n",
    "# создание узлов (+ преобразование rule_group)\n",
    "class Node:\n",
    "    __slots__ = ['childs','p_start','p_end','patterns','rule','rez','str','html']\n",
    "    def make_html(self,cols,DISTANCE):\n",
    "        l = '_'*(cols[self.p_end]-cols[self.p_start]-DISTANCE)\n",
    "        \n",
    "        patts=[]\n",
    "        elseflag = False\n",
    "        disabled_exc = False\n",
    "        for p in self.rez.parse_info.patterns:\n",
    "            if p=='__ELSE__':\n",
    "                elseflag = True\n",
    "            elif type(p)==RuleVars:\n",
    "                disabled_exc = True\n",
    "            else:\n",
    "                s = ''\n",
    "                if disabled_exc:\n",
    "                    disabled_exc = False\n",
    "                    s+='?'\n",
    "                if elseflag:\n",
    "                    elseflag = False\n",
    "                    s+='!'\n",
    "                if len(s)!=0: s+=' '\n",
    "                ps = p.__name__ if callable(p) else p['__name__']\n",
    "                if len(patts)==0:\n",
    "                    s+='<a href=\"#'+ps+'\">'+ps+'</a>'\n",
    "                else:\n",
    "                    s+='(<a href=\"#'+ps+'\">'+ps+'</a>)'\n",
    "                patts.append(s)\n",
    "        \n",
    "        p0 = patts[0]\n",
    "        r = self.rule\n",
    "        s = self.str\n",
    "        pp = '<br>'.join(patts[1:])\n",
    "        \n",
    "        self.html = l+'<br>'+p0+'<br>'+r+'<br>'+s+'<br>'+pp\n",
    "        \n",
    "def sch_make_tree(struct):\n",
    "    '''возвращает пару (глубина, [Node-ы])'''\n",
    "    if hasattr(struct,'talk'):\n",
    "        if hasattr(struct.parse_info,'p_start'):\n",
    "            info = struct.parse_info\n",
    "            node = Node()\n",
    "            node.p_start = info.p_start\n",
    "            node.p_end = info.p_end\n",
    "            node.patterns = info.patterns2str()\n",
    "            if hasattr(info,'rule_group'):\n",
    "                if type(info.rule_group)==RuleVars:\n",
    "                    assert info.rule_group[0]!=0, info.rule_group\n",
    "                    rule = info.rule_group[info.rule_group[0]]\n",
    "                    rules= str(info.rule_group[0])+'/'+str(len(info.rule_group)-1)+' '\n",
    "                else:\n",
    "                    rule = info.rule_group\n",
    "                    rules = ''\n",
    "                node.rule = rules + (rule.__name__ if callable(rule) else str(rule))\n",
    "            else:\n",
    "                node.rule = ''\n",
    "            node.rez = struct\n",
    "            node.childs = []\n",
    "            depth = 0\n",
    "            for tup in struct.talk:\n",
    "                d,m = sch_make_tree(tup[1])\n",
    "                if d>depth: depth = d\n",
    "                node.childs+=m\n",
    "            node.childs.sort(key=lambda node:node.p_start)\n",
    "            return (depth+1,[node])\n",
    "        else:\n",
    "            mm = []\n",
    "            depth = 0\n",
    "            for tup in struct.talk:\n",
    "                d,m = sch_make_tree(tup[1])\n",
    "                if d>depth: depth = d\n",
    "                mm+=m\n",
    "            return (depth,mm)\n",
    "    elif hasattr(struct.parse_info,'p_start'):\n",
    "        # получется, всё, что не является структурой - обязано содержать parse_info ?\n",
    "        info = struct.parse_info\n",
    "        node = Node()\n",
    "        node.p_start = info.p_start\n",
    "        node.p_end = info.p_end\n",
    "        node.patterns = info.patterns2str()\n",
    "        if hasattr(info,'rule_group'):\n",
    "            if type(info.rule_group)==RuleVars:\n",
    "                no = info.rule_group[0] if info.rule_group[0]!=0 else 1\n",
    "                rule = info.rule_group[no]\n",
    "                rules = str(info.rule_group[0])+'/'+str(len(info.rule_group)-1)+' '\n",
    "            else:\n",
    "                rule = info.rule_group\n",
    "                rules = ''\n",
    "            node.rule = rules + (rule.__name__ if callable(rule) else str(rule))\n",
    "        else:\n",
    "            node.rule = ''\n",
    "        node.rez = struct\n",
    "        node.childs = []\n",
    "        return (1,[node])\n",
    "    else:\n",
    "        return (0,[])\n",
    "\n",
    "def sch_print_rez1(info,depth,s):\n",
    "    '''выводит преобразованное дерево слева направо'''\n",
    "    if hasattr(info,'p_start'):\n",
    "        print('  '*depth+' '+SAttrs().join( s[info.p_start : info.p_end] ))\n",
    "    patterns = ' '.join(info.patterns) if full else info.patterns[0]\n",
    "    print('  '*depth +' '+ patterns+' -> '+info.rule)\n",
    "    print('  '*depth+'*'+str(info.rez))\n",
    "\n",
    "    if hasattr(info,'childs'):\n",
    "        for x in info.childs:\n",
    "            sch_print_rez1(x,depth+1,s)\n",
    "\n",
    "def sch_make_lines(node,lines,cols,DISTANCE):\n",
    "    '''преобразует дерево в таблицу'''\n",
    "    dd=0 # уровень, номер Лайна. У детей меньше чем у родителей.\n",
    "    for c in node.childs:\n",
    "        d = sch_make_lines(c,lines,cols,DISTANCE)\n",
    "        if d>dd: dd = d\n",
    "\n",
    "    # приводим результат к текстовой форме\n",
    "    node.str = repr(node.rez.tostr())[1:-1]\n",
    "    # приводим паттерны в надлежащий вид\n",
    "    for i in range(1,len(node.patterns)):\n",
    "        node.patterns[i] = '('+node.patterns[i]+')'\n",
    "\n",
    "    # вычисляем кол-во строк в Лайне\n",
    "    if len(node.patterns)>lines[dd].h: lines[dd].h = len(node.patterns)\n",
    "\n",
    "    # длина строки в символах\n",
    "    maxlen = max(len(node.rule),len(node.str),max([len(i) for i in node.patterns]))\n",
    "    dlen = (maxlen+DISTANCE) - (cols[node.p_end]-cols[node.p_start])\n",
    "    #print(node.str,maxlen,dlen)\n",
    "    # модифицируем позиции столбцов\n",
    "    if dlen>0:\n",
    "        for i in range(node.p_end,len(cols)):\n",
    "            cols[i]+=dlen\n",
    "\n",
    "    # присваиваем объект в таблицу\n",
    "    lines[dd].line[node.p_start] = node\n",
    "    return dd+1\n",
    "\n",
    "def sch_print_table(s,cols,lines,DISTANCE,detailed):\n",
    "    def inde(s,p_start,p_end):\n",
    "        '''дополняет строку пробелами, чтобы заполнить нужные ячейки'''\n",
    "        return s+' '*(cols[p_end]-cols[p_start]-len(s))\n",
    "\n",
    "    # выводим исходную строку\n",
    "    for i in range(len(s)):\n",
    "        print(inde(s[i],i,i+1),end='')\n",
    "    print()\n",
    "\n",
    "    for line_ in lines:\n",
    "        line=line_.line\n",
    "\n",
    "        # _______\n",
    "        i=0\n",
    "        l=''\n",
    "        while i<len(s):\n",
    "            if line[i]==None:\n",
    "                l+=inde('',i,i+1)\n",
    "                i+=1\n",
    "            else:\n",
    "                l+='_'*(cols[line[i].p_end]-cols[line[i].p_start]-DISTANCE)+' '*DISTANCE\n",
    "                i=line[i].p_end\n",
    "        print(l)\n",
    "\n",
    "        # patterns[0]\n",
    "        i=0\n",
    "        l=''\n",
    "        while i<len(s):\n",
    "            if line[i]==None:\n",
    "                l+=inde('',i,i+1)\n",
    "                i+=1\n",
    "            else:\n",
    "                l+=inde(line[i].patterns[0],line[i].p_start,line[i].p_end)\n",
    "                i=line[i].p_end\n",
    "        print(l)\n",
    "\n",
    "        # rule\n",
    "        i=0\n",
    "        l=''\n",
    "        while i<len(s):\n",
    "            if line[i]==None:\n",
    "                l+=inde('',i,i+1)\n",
    "                i+=1\n",
    "            else:\n",
    "                l+=inde(line[i].rule,line[i].p_start,line[i].p_end)\n",
    "                i=line[i].p_end\n",
    "        print(l)\n",
    "\n",
    "        # rez\n",
    "        i=0\n",
    "        l=''\n",
    "        while i<len(s):\n",
    "            if line[i]==None:\n",
    "                l+=inde('',i,i+1)\n",
    "                i+=1\n",
    "            else:\n",
    "                l+=inde(line[i].str,line[i].p_start,line[i].p_end)\n",
    "                i=line[i].p_end\n",
    "        print(l)\n",
    "\n",
    "        # patterns[j]\n",
    "        if detailed>=1:\n",
    "            for j in range(1,line_.h):\n",
    "                i=0\n",
    "                l=''\n",
    "                while i<len(s):\n",
    "                    if line[i]==None or j>=len(line[i].patterns):\n",
    "                        l+=inde('',i,i+1)\n",
    "                        i+=1\n",
    "                    else:\n",
    "                        l+=inde(line[i].patterns[j],line[i].p_start,line[i].p_end)\n",
    "                        i=line[i].p_end\n",
    "                print(l)\n",
    "\n",
    "def scheme(s,detailed=1,nohtml = False):\n",
    "    '''печатает схему разбора\n",
    "    \n",
    "    есть дополнительный аргумент datailed\n",
    "        0 - не печатает сквозные паттерны (которые без правил)\n",
    "        1 - дефолтный вывод\n",
    "        2 - дополнительно печатает нестрингифицированные объекты\n",
    "    '''\n",
    "    # токенизируем строку\n",
    "    s=[ i for i in tokenizer(s)]\n",
    "    \n",
    "    # парсим строку\n",
    "    ParseInfo.enabled = True\n",
    "    try:\n",
    "        rezs=maxlen_filter(alt(p_sentence,p_phrase,p_question_phrase),s,0)\n",
    "    finally:\n",
    "        ParseInfo.enabled = False\n",
    "        \n",
    "    hstr = ''\n",
    "    def h_print(s=''):\n",
    "        nonlocal nohtml\n",
    "        if nohtml:\n",
    "            print(s)\n",
    "        else:\n",
    "            nonlocal hstr\n",
    "            hstr+=s+'\\n'\n",
    "        \n",
    "    h_print(str(len(rezs))+' результатов')\n",
    "    # анализируем каждый результат\n",
    "    for end,rez in rezs:\n",
    "        h_print()\n",
    "        # простой построчный вывод узлов результата\n",
    "        if detailed==2:\n",
    "            rez.pull_deferred()\n",
    "            sch_print_rez0(rez,0,s,detailed)\n",
    "        \n",
    "        # простой вывод узлов\n",
    "        if 0:\n",
    "            rez.pull_deferred()\n",
    "            \n",
    "        # создание дерева\n",
    "        depth,mm = sch_make_tree(rez)\n",
    "        assert len(mm)==1\n",
    "        tree=mm[0]\n",
    "        \n",
    "        # простой вывод узлов\n",
    "        if 0:\n",
    "            print('depth=',depth)\n",
    "            sch_print_rez1(tree,0,s)\n",
    "        \n",
    "        DISTANCE = 2\n",
    "        cols = [] # позиции столбцов - кол-во символов от начала строки до начала столбца\n",
    "        pos=0\n",
    "        for word in s:\n",
    "            cols.append(pos)\n",
    "            pos+=len(word)+DISTANCE\n",
    "        cols.append(pos)\n",
    "        \n",
    "        class Line:\n",
    "            __slots__=['h','line']\n",
    "            def __init__(self,h,l):\n",
    "                self.h = h      # количество строчек в чинии\n",
    "                self.line = l   # список объектов (Node-ов) по столбцам\n",
    "                \n",
    "        lines = [Line(0,[None for i in range(len(s))]) for j in range(depth)]\n",
    "        dd = sch_make_lines(tree,lines,cols,DISTANCE)\n",
    "        assert dd==depth\n",
    "        \n",
    "        if nohtml:\n",
    "            sch_print_table(s,cols,lines,DISTANCE,detailed)\n",
    "        \n",
    "        else:\n",
    "            def make_html(n):\n",
    "                n.make_html(cols,DISTANCE)\n",
    "                for i in n.childs:\n",
    "                    make_html(i)\n",
    "            make_html(tree)\n",
    "\n",
    "            ss = '<tr>'+''.join(['<td>'+si+'</td>' for si in s])+'</tr>\\n'\n",
    "            for ll in lines:\n",
    "                sl = '<tr>\\n'\n",
    "                p_end=0\n",
    "                for l in ll.line:\n",
    "                    if l!=None:\n",
    "                        sl+='<td></td>'*(l.p_start-p_end)\n",
    "                        sl+='<td colspan=\"'+str(l.p_end-l.p_start)+'\">'+l.html+'</td>\\n'\n",
    "                        p_end = l.p_end\n",
    "                ss+=sl+'</tr>\\n\\n'\n",
    "\n",
    "            h_print('<table>'+ss+'\\n</table>\\n')\n",
    "\n",
    "    if not nohtml:\n",
    "        return HTML(hstr)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## составляем основную грамматику"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "выбираем варианты вручную\n",
    "пока в одном тексте не начнут требоваться разные варианты одного правила\n",
    "\n",
    "на боевое применение не выходим\n",
    "пока не будут пройдены времена глаголов\n",
    "\n",
    "КОДИТЬ И ДЕБАЖИТЬ ТЕКУЩЕЕ\n",
    "13) где?\n",
    "14) какого цвета?\n",
    "15) посмотри\n",
    "16) что у тебя есть?\n",
    "17) что это за <животное>?\n",
    "18) Polly's quail\n",
    "19) как тебя зовут?\n",
    "20) \n",
    "21) сколько тебе лет?\n",
    "22) take, give...\n",
    "23) can\n",
    "24) may, must\n",
    "25) must\n",
    "26) there is\n",
    "27) is there?\n",
    "28)\n",
    "29) когда? тогда\n",
    "30) рифма\n",
    "31) сравнительны прилагательные\n",
    "32) другая рифма\n",
    "33) скороговорка\n",
    "34) \n",
    "35)\n",
    "36)\n",
    "37)\n",
    "38) мой, твой\n",
    "39)\n",
    "стандартные фразы\n",
    "3 рассказа\n",
    "\n",
    "автовыбор склонений/спряжений прилагательных и глаголов\n",
    "сделать устранение конфликтов исключений\n",
    "\n",
    "\n",
    "два рыжих кота\n",
    "вижу двух рыжих котов\n",
    "вижу два горячих утюга\n",
    "два пирожных\n",
    "\n",
    "add_skl_suffix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## придумываем способ выборов вариантов (контекстных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "\n",
    "git-ветка с исключениями\n",
    "    исключения на несколько предложений\n",
    "     - не правильно, контексты - не синтаксические конструкции\n",
    "git-ветка с нейросетью\n",
    "    научится...\n",
    "git-ветка с другим жестким алгоритмом\n",
    "    придумать...\n",
    "\n",
    "    работа с деревом вглубь:\n",
    "        просмотр вглубь возможен\n",
    "\n",
    "        у каждого узла ссылка на правило и его аргументы\n",
    "        в узлах дерева поля \n",
    "            context_dep\n",
    "                True - узел зависит от контекста\n",
    "                    ссылка на правило, также принимат контекст\n",
    "                False - узел не зависит от контекста\n",
    "            context_dep_srcs - массив номеров - \n",
    "                какие аргументы правила зависят от контекста (или их потомки зависят от контекста)\n",
    "                т.е. какие аргументы правила требую ремейка в случае изменения контекста\n",
    "\n",
    "            context(может отсутствовать) - словарь (строка, ссылка на узел), который является контекстом\n",
    "                - устанавливается в правилах\n",
    "        функция context_remake(node,context)\n",
    "\n",
    "статистика использования паттернов\n",
    "статистика повторяющихся структур в тестах (разделить тесты на несвязанные предложения)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## __ по ходу дела по мере необходимости улучшаем гуй, в jupyter notebook-е\n",
    "виджетами и прочим  \n",
    "а также добавляем возможность переводить html и tex\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## делаем web-GUI на javascript-е\n",
    "\n",
    "выбор паттернов и правил в зависимости от времени "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "watch, двое, трое, пятеро\n",
    "\n",
    "scheme('it')\n",
    "\n",
    "...\n",
    "для больших текстов p_sentence будет делать срез со своей позции до конца\n",
    "    - чтобы обновить кэши ф-ций\n",
    "\n",
    "исключения парсить, если регулярным образом распарсилось\n",
    "    каждая функция будет с аргументом: парсить или нет исключения\n",
    "\n",
    "атрибуты слов: (теги)\n",
    "отображение открывающейся кавычки (SAttrs.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:04.908199Z",
     "start_time": "2019-08-13T17:27:04.807193Z"
    }
   },
   "outputs": [],
   "source": [
    "#!jupyter nbconvert --to script en2ru.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:05.026206Z",
     "start_time": "2019-08-13T17:27:04.914200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Я вижу джем и одну чашку.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ru('I see jam and one cup.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:05.128212Z",
     "start_time": "2019-08-13T17:27:05.031206Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#decline('two watches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:05.609239Z",
     "start_time": "2019-08-13T17:27:05.132212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''У неё есть шляпа; она тоже есть белый.\n",
      "У неё есть лента; она -- красная.'''\n"
     ]
    }
   ],
   "source": [
    "!!!\n",
    "pr_l_repr(en2ru_with_variants([\n",
    "    (dict_pronoun_ip['it'],3),\n",
    "],'''She has a hat; it is white too.\n",
    "She has a ribbon; it is red.\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:25:41.641345Z",
     "start_time": "2019-08-13T18:25:41.625344Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1 результатов\n",
       "\n",
       "<table><tr><td>good</td><td>morning</td></tr>\n",
       "<tr>\n",
       "<td colspan=\"2\">______________<br>! <a href=\"#p_adj_noun3\">p_adj_noun3</a><br>r_GOOD_MORNING<br>доброе утро<br>(<a href=\"#p_noun3\">p_noun3</a>)<br>(<a href=\"#p_noun2_1\">p_noun2_1</a>)<br>(<a href=\"#p_noun2\">p_noun2</a>)<br>(<a href=\"#p_noun1\">p_noun1</a>)<br>(<a href=\"#p_noun\">p_noun</a>)<br>(<a href=\"#p_phrase\">p_phrase</a>)</td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme('good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T18:25:43.337442Z",
     "start_time": "2019-08-13T18:25:43.312440Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1 результатов\n",
       "\n",
       "<table><tr><td>you</td><td>have</td><td>one</td><td>hen</td><td>and</td><td>eight</td><td>chickens</td><td>.</td></tr>\n",
       "<tr>\n",
       "<td colspan=\"1\">_______________<br><a href=\"#dict_pronoun_ip\">dict_pronoun_ip</a><br>1/3 ты<br>Ты<br>(<a href=\"#p_noun1_ip\">p_noun1_ip</a>)<br>(<a href=\"#p_noun_ip\">p_noun_ip</a>)</td>\n",
       "<td></td><td colspan=\"1\">____________<br><a href=\"#dict_numeral\">dict_numeral</a><br>1/3 один<br>один<br>(<a href=\"#p_numeral\">p_numeral</a>)</td>\n",
       "<td colspan=\"1\">___________<br><a href=\"#dict_noun\">dict_noun</a><br>1/1 курица<br>курица<br>(<a href=\"#p_noun3\">p_noun3</a>)<br>(<a href=\"#p_noun2_1\">p_noun2_1</a>)<br>(<a href=\"#p_noun2\">p_noun2</a>)</td>\n",
       "<td></td><td colspan=\"1\">____________<br><a href=\"#dict_numeral\">dict_numeral</a><br>восемь<br>восемь<br>(<a href=\"#p_numeral\">p_numeral</a>)</td>\n",
       "<td colspan=\"1\">___________<br><a href=\"#dict_noun\">dict_noun</a><br>1/1 цыплята<br>цыплята<br>(<a href=\"#p_noun3\">p_noun3</a>)<br>(<a href=\"#p_noun2_1\">p_noun2_1</a>)<br>(<a href=\"#p_noun2\">p_noun2</a>)</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td></td><td></td><td colspan=\"2\">_________________________<br><a href=\"#p_noun1\">p_noun1</a><br>r_numeral_noun<br>одна курица<br></td>\n",
       "<td></td><td colspan=\"2\">_________________________<br><a href=\"#p_noun1\">p_noun1</a><br>r_numeral_noun<br>восемь цыплят<br>(<a href=\"#p_noun\">p_noun</a>)</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td></td><td></td><td colspan=\"5\">_________________________________________________________<br><a href=\"#p_noun\">p_noun</a><br>r_noun_and_noun<br>одна курица и восемь цыплят<br></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td colspan=\"7\">________________________________________________________________________________<br><a href=\"#pe_noun_HAVE_noun\">pe_noun_HAVE_noun</a><br>2/2 r_U_noun_EST_noun<br>у Тебя есть одна курица и восемь цыплят<br>! (<a href=\"#p_noun_verb1\">p_noun_verb1</a>)<br>(<a href=\"#p_verb1\">p_verb1</a>)<br>(<a href=\"#p_verb\">p_verb</a>)<br>(<a href=\"#p_phrase\">p_phrase</a>)</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "<td colspan=\"8\">___________________________________________________________________________________<br><a href=\"#p_sentence\">p_sentence</a><br>r_sentence<br>У тебя есть одна курица и восемь цыплят.<br></td>\n",
       "</tr>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheme('''You have one hen and eight chickens.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:07.438344Z",
     "start_time": "2019-08-13T17:27:06.124269Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lesson 1 ---\n",
      "--- Lesson 2 ---\n",
      "--- Lesson 3 ---\n",
      "--- Lesson 4 (? увидь кота ?) ---\n",
      "--- Lessons 5, 6 ---\n",
      "--- Lesson 7 ---\n",
      "--- Lesson 8 (открывающие кавычки...) ---\n",
      "--- Lesson 8.1 ---\n",
      "--- Lesson 9 ---\n",
      "--- Lesson 10 ---\n",
      "--- Lesson 11 ---\n",
      "--- Lesson 12 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tests\n",
    "tests = reload(tests)\n",
    "tests.init(parse_system,en_dictionary,\n",
    "           en2ru,en2ru_with_variants,decline,scheme,d_en2ru,pr_l_repr,\n",
    "           p_noun,p_noun1,r_noun_comma_noun,rv_noun_HAVE_noun,\n",
    "          1,False)\n",
    "tests.test1()\n",
    "tests.test2()\n",
    "tests.test3()\n",
    "tests.test4()\n",
    "tests.test5and6()\n",
    "tests.test7()\n",
    "tests.test8()\n",
    "tests.test8_1()\n",
    "tests.test9()\n",
    "tests.test10()\n",
    "tests.test11()\n",
    "tests.test12()\n",
    "tests.finalize()\n",
    "tests.TEST_ERRORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-13T17:27:07.450345Z",
     "start_time": "2019-08-13T17:27:07.442344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"#Запуск-и-отладка\">link</a>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<a href=\"#Запуск-и-отладка\">link</a>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
